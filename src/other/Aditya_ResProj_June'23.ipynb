{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNqOSgRsfmzI"
      },
      "source": [
        "# **IMPORTS AND LOADING HELPER FILES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNpgShlblPzi"
      },
      "source": [
        "I run this notebook on Google Colab. Please upload the colabTest.zip file before running the first cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-s8s8a8z9uR"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZm-68Ixzz3e",
        "outputId": "261eaaf2-7ada-4ad8-f29f-269182a77fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  colabTest.zip\n",
            "  inflating: Final_50_Sample.csv     \n",
            "  inflating: utils.py                \n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "sqlite3 is already the newest version (3.37.2-2ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Cloning into 'openai-python'...\n",
            "remote: Enumerating objects: 1576, done.\u001b[K\n",
            "remote: Counting objects: 100% (458/458), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 1576 (delta 396), reused 353 (delta 348), pack-reused 1118\u001b[K\n",
            "Receiving objects: 100% (1576/1576), 1.88 MiB | 5.57 MiB/s, done.\n",
            "Resolving deltas: 100% (943/943), done.\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.8.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n",
            "python3: can't open file '/content/setup.py': [Errno 2] No such file or directory\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.3)\n",
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.1\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!unzip colabTest.zip\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "!apt install sqlite3\n",
        "df = pd.read_csv('./Final_50_Sample.csv')\n",
        "from os.path import join\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from utils import *\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from utils import *\n",
        "from itertools import compress\n",
        "\n",
        "import os\n",
        "import json\n",
        "map_func = {'sum': np.sum, 'avg': np.mean, 'max': np.max, 'count': len}\n",
        "\n",
        "!git clone https://github.com/openai/openai-python.git\n",
        "!cd openai-python\n",
        "!pip install aiohttp\n",
        "!python setup.py install\n",
        "!pip install tenacity\n",
        "import numpy as np\n",
        "!pip install openai\n",
        "import openai\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "openai.api_key = \"YOUR OPENAI KEY\"\n",
        "from math import ceil\n",
        "from time import sleep\n",
        "\n",
        "import subprocess\n",
        "import duckdb\n",
        "import subprocess\n",
        "import os\n",
        "!pip install rouge\n",
        "from rouge import Rouge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-sdfiMqaeLh"
      },
      "source": [
        "## **QueryTree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zWUhCMN1as14"
      },
      "outputs": [],
      "source": [
        "box_head1 = '┌───────────────────────────┐'\n",
        "\n",
        "box_head2 = '┌─────────────┴─────────────┐'\n",
        "\n",
        "box_end1  = '└─────────────┬─────────────┘'\n",
        "\n",
        "box_end2  = '└───────────────────────────┘'\n",
        "\n",
        "class Node:\n",
        "    def __init__(self):\n",
        "        self.text, self.l, self.r, self.op, self.args = None, None, None, None, None\n",
        "        self.questions, self.unfiltered_answers, self.answers = [], [], []\n",
        "\n",
        "def parse_query_tree(s):\n",
        "    root = Node()\n",
        "    p=root\n",
        "    for ss in s:\n",
        "        if ss ==box_head1 or ss== box_head2:\n",
        "          seach_text=True\n",
        "          text=[]\n",
        "        elif  box_end1 in ss or box_end2 in ss:\n",
        "          search_text=False\n",
        "          p.text = text\n",
        "          p.op = p.text[0]\n",
        "          p.args = p.text[1:]\n",
        "          p.l=Node()\n",
        "          p=p.l\n",
        "        else:\n",
        "          t=ss.replace('|','').replace('─','').replace('│','').replace('└┬┘','').replace('└┘','').strip()#.replace(' ','')\n",
        "          if t:\n",
        "              text.append(t)\n",
        "              if ')' in text[-1] and '(' not in text[-1]: text = text[:-2]+[\" \".join(text[-2:])]\n",
        "    # remove projection after aggregation\n",
        "    if root.op=='PROJECTION' and root.l and root.l.op=='AGGREGATE' and root.args == root.l.args: root=root.l\n",
        "    return root\n",
        "\n",
        "\n",
        "def parse_query_tree(s):\n",
        "    print(s)\n",
        "    root = Node()  # Create the root node\n",
        "    p = root  # Set the current node as the root\n",
        "\n",
        "    for ss in s:\n",
        "        if ss == box_head1 or ss == box_head2:  # Check if ss matches box_head1 or box_head2\n",
        "            search_text = True  # Set a flag to indicate searching for text\n",
        "            text = []  # Initialize an empty list to store text\n",
        "        elif box_end1 in ss or box_end2 in ss:  # Check if box_end1 or box_end2 is present in ss\n",
        "            search_text = False  # Set the flag to indicate the end of searching for text\n",
        "            p.text = text  # Assign the collected text to the current node\n",
        "            p.op = p.text[0]  # Assign the operation to the current node\n",
        "            p.args = p.text[1:]  # Assign the arguments to the current node\n",
        "            p.l = Node()  # Create a new node and assign it as the left child of the current node\n",
        "            p = p.l  # Update the current node to the left child\n",
        "        else:\n",
        "            t = ss.replace('|','').replace('─','').replace('│','').replace('└┬┘','').replace('└┘','').strip()\n",
        "            # Clean up the string ss and remove unnecessary characters\n",
        "\n",
        "            if t:\n",
        "                text.append(t)  # Append the cleaned text to the list\n",
        "\n",
        "                if ')' in text[-1] and '(' not in text[-1]:\n",
        "                    # Check if the last element in text contains a closing parenthesis without an opening parenthesis\n",
        "                    text = text[:-2] + [\" \".join(text[-2:])]  # Merge the last two elements in text with a space\n",
        "\n",
        "    # Remove projection after aggregation\n",
        "    if root.op == 'PROJECTION' and root.l and root.l.op == 'AGGREGATE' and root.args == root.l.args:\n",
        "        root = root.l  # Update the root to the left child\n",
        "\n",
        "    return root  # Return the root node\n",
        "\n",
        "\n",
        "def print_tree(p):\n",
        "    if p and p.text:\n",
        "      print_tree(p.l)\n",
        "      print_tree(p.r)\n",
        "\n",
        "def get_tree_elements(p, a):\n",
        "    if p and p.text:\n",
        "        a.append(p.text)\n",
        "\n",
        "        # Recursively call get_tree_elements on p's left and right child\n",
        "        get_tree_elements(p.l, a)\n",
        "        get_tree_elements(p.r, a)\n",
        "\n",
        "    return a\n",
        "\n",
        "\n",
        "def get_snippet(p, tree_nodes, questions, answers, unfiltered_answers):\n",
        "    \"\"\"\n",
        "    Recursively traverses a tree and extracts relevant data into separate lists.\n",
        "\n",
        "    Args:\n",
        "        p (Node): Current node in the tree\n",
        "        tree_nodes (list): List to store adjusted nodes\n",
        "        questions (list): List to store questions\n",
        "        answers (list): List to store answers\n",
        "        unfiltered_answers (list): List to store unfiltered answers\n",
        "\n",
        "    Returns:\n",
        "        tuple: Lists containing adjusted nodes, questions, answers, and unfiltered answers\n",
        "    \"\"\"\n",
        "    if p and p.text:\n",
        "        # Traverse the left subtree\n",
        "        get_snippet(p.l, tree_nodes, questions, answers, unfiltered_answers)\n",
        "        # Traverse the right subtree\n",
        "        get_snippet(p.r, tree_nodes, questions, answers, unfiltered_answers)\n",
        "\n",
        "        # Append data from the current node to respective lists\n",
        "        tree_nodes.append(p.adjusted_nodes)\n",
        "        questions.append(p.questions)\n",
        "        answers.append(p.answers)\n",
        "        unfiltered_answers.append(p.unfiltered_answers)\n",
        "\n",
        "    # Return the collected data as tuples\n",
        "    return tree_nodes, questions, answers, unfiltered_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_DgWzPcak03"
      },
      "source": [
        "## **ChatGPT Calls**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EnyTnxOd3Sdu"
      },
      "outputs": [],
      "source": [
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        "    wait_fixed\n",
        ")\n",
        "import openai\n",
        "@retry(wait=wait_fixed(61), stop=stop_after_attempt(3))\n",
        "def completion_with_backoff_chat(**kwargs):\n",
        "    #print(f\"Value of kwaargs: {kwargs}\")\n",
        "    #print(f\"{type(**kwargs)}\")\n",
        "    #print('=======Calling API=======')\n",
        "    #sleep(21)\n",
        "    return openai.ChatCompletion.create(**kwargs)\n",
        "\n",
        "def construct_chat_dict(role,content):\n",
        "  return {\"role\":role,\"content\":content}\n",
        "\n",
        "def construct_message_dict(instruction, question_answers):\n",
        "    messages = []\n",
        "\n",
        "    # Add system instruction to the list of messages\n",
        "    messages.append(construct_chat_dict(\"system\", instruction))\n",
        "\n",
        "    # Iterate through each question-answer pair\n",
        "    for question, answer in question_answers:\n",
        "        # Add user's question to the list of messages\n",
        "        messages.append(construct_chat_dict(\"user\", question))\n",
        "        # Add assistant's answer to the list of messages\n",
        "        messages.append(construct_chat_dict(\"assistant\", answer))\n",
        "\n",
        "    return messages\n",
        "\n",
        "def add_more_seq_scan(temp_unfilt_ans, model_arch, temp_questions, old_pr, old_ans, max_tries=0, increase_threshold=5, verbose=True):\n",
        "    i = 0\n",
        "    final_ans = old_ans\n",
        "\n",
        "    while i < max_tries:\n",
        "        if verbose:\n",
        "            print('#', i + 1)\n",
        "\n",
        "        # Create the prompt for the GPT-3 API\n",
        "        pr = old_pr + [construct_chat_dict(\"assistant\", old_ans)] + [construct_chat_dict(\"user\", 'Give me more.')]\n",
        "        messages = pr\n",
        "        temp_questions.append(\"Give me more.\")\n",
        "\n",
        "        # Run GPT-3 to generate a response\n",
        "        max_len = 400\n",
        "        response = completion_with_backoff_chat(model=model_arch, messages=messages, temperature=0, max_tokens=max_len)\n",
        "\n",
        "        ans = response['choices'][0]['message']['content']\n",
        "\n",
        "        if verbose:\n",
        "            print(ans)\n",
        "        temp_unfilt_ans.append(ans)\n",
        "\n",
        "        final_ans += ans\n",
        "\n",
        "        len1 = len(set(old_ans.split(',')))\n",
        "        len2 = len(set(old_ans.split(',') + ans.split(',')))\n",
        "\n",
        "        old_ans = ans\n",
        "        old_pr = pr\n",
        "\n",
        "        if (len2 - len1) < increase_threshold:\n",
        "            break\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return final_ans\n",
        "\n",
        "\n",
        "def answer_batch_questions_chat(model_questions,pr,label,cache_fn,model_arch,max_len,verbose=False):\n",
        "    cache_fn=label+'_cache.json'\n",
        "    mode = 'r' if cache_fn in os.listdir('.') else 'w'\n",
        "    cache=json.load(open(cache_fn,'r')) if mode=='r' else dict()\n",
        "    ans=[]\n",
        "    batch_pr = pr[:]\n",
        "    batch_mq = model_questions[:]\n",
        "    batch_ans = [None]*len(batch_pr)\n",
        "    to_fetch_indices=[]\n",
        "\n",
        "    for ind,pro in enumerate(batch_pr):\n",
        "        pro_key = json.dumps(pro) if isinstance(pro,list) else pro\n",
        "        if pro_key in cache:\n",
        "            #print('Already in cache: ',batch_mq[ind],'\\n')\n",
        "            batch_ans[ind]=cache[pro_key]\n",
        "        else:\n",
        "            to_fetch_indices.append(ind)\n",
        "    #print('INITIAL BATCH_ANS:',batch_ans)\n",
        "    if verbose:\n",
        "        print(f'In Cache: {len(batch_ans)-len(to_fetch_indices)}/{len(batch_ans)}')\n",
        "    if to_fetch_indices:\n",
        "        batch_pr_to_fetch = [batch_pr[tfi] for tfi in to_fetch_indices]\n",
        "        batch_mq_to_fetch = [batch_mq[tfi] for tfi in to_fetch_indices]\n",
        "\n",
        "        for i in range(len(batch_pr_to_fetch)):\n",
        "            #print(batch_pr_to_fetch[i])\n",
        "            response = completion_with_backoff_chat(model=model_arch, messages=batch_pr_to_fetch[i], temperature=0,max_tokens= max_len)\n",
        "\n",
        "            if 'choices' in response:\n",
        "                #print('RESPONSE:',response)\n",
        "                batch_ans_fetched = response['choices'][0]['message']['content']\n",
        "\n",
        "                #print('BATCH ANS FETCHED:',batch_ans_fetched)\n",
        "\n",
        "\n",
        "                bpr = batch_pr_to_fetch[i]\n",
        "                bmq= batch_mq_to_fetch[i]\n",
        "                bans= batch_ans_fetched\n",
        "                bpr_key = json.dumps(bpr) if isinstance(bpr,list) else bpr\n",
        "                cache[bpr_key]=bans\n",
        "                print('Added to cache:',bmq,'\\n\\n')\n",
        "                print(f'Len of cache: {len(cache)}')\n",
        "                json.dump(cache,open(cache_fn,\"w\"),indent=2)\n",
        "            else: batch_ans_fetched=[]\n",
        "\n",
        "            batch_ans[to_fetch_indices[i]] =batch_ans_fetched\n",
        "\n",
        "        ans.extend(batch_ans)\n",
        "    return batch_ans\n",
        "\n",
        "\n",
        "def compute_node(node,model_arch,instr,few_shots,inst_funct,label,augmented_question_maps,verbose=False):\n",
        "\n",
        "    cache_fn=label+'_cache.json'\n",
        "    mode = 'r' if cache_fn in os.listdir('.') else 'w'\n",
        "    if verbose: print('Mode: ',mode)\n",
        "    cache=json.load(open(cache_fn,'r')) if mode=='r' else dict()\n",
        "    #print(f'Len of cache: {len(cache)}')\n",
        "    status = 'FINISHED'\n",
        "\n",
        "    if node.op=='JOIN':\n",
        "        #pr= instr+few_shots+\"\"\n",
        "        pr=construct_message_dict(instr,few_shots)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        left_questions = [node.key_left.replace('!!x!!',x) for x in node.l.answers[-1]]\n",
        "        print('left questions',left_questions)\n",
        "\n",
        "\n",
        "        #lpr = [pr + inst_funct(x) for x in left_questions]\n",
        "        lpr = [pr + [ construct_chat_dict(\"user\",x)] for x in left_questions]\n",
        "\n",
        "        #batch_left_ans = answer_batch_questions(left_questions,lpr,label,cache_fn,model_arch,50,verbose=verbose)\n",
        "\n",
        "        batch_left_ans = answer_batch_questions_chat(left_questions,lpr,label,cache_fn,model_arch,50,verbose=verbose)\n",
        "\n",
        "        print('left answer',batch_left_ans)\n",
        "\n",
        "        right_questions = [node.key_right.replace('!!x!!',x) for x in node.r.answers[-1]]\n",
        "        print('right questions',right_questions)\n",
        "\n",
        "        #rpr = [pr + inst_funct(x) for x in right_questions]\n",
        "        rpr = [pr + [ construct_chat_dict(\"user\",x)] for x in right_questions]\n",
        "\n",
        "\n",
        "\n",
        "        #batch_right_ans = answer_batch_questions(right_questions,rpr,label,cache_fn,model_arch,400,verbose=verbose)\n",
        "        batch_right_ans = answer_batch_questions_chat(right_questions,rpr,label,cache_fn,model_arch,50,verbose=verbose)\n",
        "\n",
        "        print('right answer',batch_right_ans)\n",
        "\n",
        "        left = pd.DataFrame({'left':node.l.answers[-1],\"key\":batch_left_ans})\n",
        "\n",
        "        right = pd.DataFrame({'right':node.r.answers[-1],\"key\":batch_right_ans})\n",
        "        print(f\"Left is: {left}\")\n",
        "        print(f\"Right is: {right}\")\n",
        "        print(left.merge(right,on='key',how='inner'))\n",
        "        ans = list (left.merge(right,on='key',how='inner')[node.filter_key])\n",
        "        print('JOINED ANSWER',ans)\n",
        "        node.answers = ans\n",
        "        node.status = status\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    node_text_list = node.text\n",
        "    print(node_text_list)\n",
        "    adjusted_nodes_list = node.adjusted_nodes\n",
        "    if verbose:\n",
        "        print('Tree Nodes: ',([\"_\".join(x) for x in adjusted_nodes_list]))\n",
        "\n",
        "    for adjusted_node in  adjusted_nodes_list:\n",
        "        op = adjusted_node[0]\n",
        "\n",
        "        if 'AGGREGATE_count_star()' in \"_\".join(adjusted_node) or 'AGGREGATE_OP_count' in \"_\".join(adjusted_node):\n",
        "            try:\n",
        "\n",
        "                ans = len(node.answers[-1] if len(adjusted_nodes_list)>1 else node.l.answers[-1])\n",
        "\n",
        "            except:\n",
        "                status = 'FAILED AGGREGATE OPERATION'\n",
        "                ans=[]\n",
        "\n",
        "            #temp_questions.append('COUNT')\n",
        "            node.questions.append('COUNT')\n",
        "            node.status = status\n",
        "            #temp_unfilt_ans.append(ans)\n",
        "            node.unfiltered_answers.append(ans)\n",
        "\n",
        "            #temp_ans.append(ans)\n",
        "            node.answers.append(ans)\n",
        "\n",
        "            if verbose:\n",
        "                print('Answer: ',ans)\n",
        "\n",
        "\n",
        "        elif 'AGGREGATE_OP' in op:\n",
        "\n",
        "            func = adjusted_node[1]\n",
        "            #proj_ind = node_ind-1\n",
        "\n",
        "            #if verbose:\n",
        "            #    print('PROJ IND:',proj_ind)\n",
        "            #    print('Answers: ',temp_ans[proj_ind])\n",
        "\n",
        "            #temp_questions.append(func+'('+\",\".join(temp_ans[proj_ind])+')')\n",
        "            node.questions.append(func+'('+\",\".join(node.answers[-1])+')')\n",
        "\n",
        "\n",
        "\n",
        "            #prev_ans=[]\n",
        "\n",
        "            try:\n",
        "                #prev_ans = [x[:-1] if x[-1]=='.' else x for x in temp_ans[proj_ind]]\n",
        "\n",
        "                prev_ans = node.l.ans[-1]\n",
        "                prev_ans = [x[:-1] if x[-1]=='.' else x for x in prev_ans]\n",
        "\n",
        "                if func!='count':\n",
        "                    numer_ans = [replace_units(x) for x in prev_ans]\n",
        "                    #print('Replacing UNITS:',numer_ans)\n",
        "                    numer_ans = [re.sub(\"[^0-9.*]\",\"\",x) for x in numer_ans]\n",
        "                    #print('Remove other text:',numer_ans)\n",
        "                    numer_ans = [x for x in numer_ans if x]\n",
        "                    numer_ans = [x[:-1] if x[-1]=='.' else x for x in numer_ans]\n",
        "                    numer_ans = [x for x in numer_ans if x]\n",
        "                    numer_ans = [eval(x) for x in numer_ans]\n",
        "                    #print('Evaluate numbers:',numer_ans)\n",
        "                else:\n",
        "                    numer_ans = [x for x in numer_ans if x]\n",
        "                #if verbose:\n",
        "                    #print('Removing non-nuemrical: ',numer_ans)\n",
        "                if func!='count':\n",
        "                    numer_ans = [ float(x) for x in numer_ans]\n",
        "                if verbose:\n",
        "                    print('Numerical Parsing: ',numer_ans)\n",
        "                ans = map_func(func)(numer_ans)\n",
        "                node.status = 'FINISHED'\n",
        "\n",
        "            except:\n",
        "                node.status = 'FAILED AGGREGATE OPERATION'\n",
        "                ans=[]\n",
        "\n",
        "            #temp_unfilt_ans.append(ans)\n",
        "            node.unfiltered_answers.append(ans)\n",
        "            #temp_ans.append(ans)\n",
        "            node.answers.append(ans)\n",
        "\n",
        "            if verbose:\n",
        "            #    print('Function used: ',map_func(func))\n",
        "                print('Unfiltered answer:',ans)\n",
        "                print('Filtered answer:',ans)\n",
        "                print('Status: ',status)\n",
        "        else:\n",
        "            k = '_'.join(adjusted_node)\n",
        "            if hasattr(node, 'filled_questions'):\n",
        "              question = node.filled_question + \"Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\"\n",
        "            elif k in augmented_question_maps:\n",
        "              question = augmented_question_maps[k] + \"Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\"\n",
        "              print(f\"k exists! k value is {k}\")\n",
        "            else:\n",
        "              question = prompt_create(dfTrain, k)['choices'][0]['message']['content'] + \" Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\"\n",
        "              # print(f\"question is: {question['choices'][0]['message']['content']}\")\n",
        "            #question = node.filled_question if hasattr(node,'filled_question') else augmented_question_maps[k]\n",
        "            #print(f\"Converted Question: {question}\")\n",
        "            if verbose:\n",
        "                print('OP: ',k)\n",
        "                print('Q: ',question)\n",
        "\n",
        "            # adjust max length generation\n",
        "            if op == 'SEQ_SCAN':max_len = 400\n",
        "            elif op=='FILTER': max_len=1 if 'turbo' not in model_arch else 2\n",
        "            else: max_len = 50\n",
        "\n",
        "            #proj_ind = -1\n",
        "\n",
        "            if op=='AGGREGATE_PROJ':\n",
        "                prev_ans = node.l.answers[-1]\n",
        "            elif  op =='PROJECTION':\n",
        "                prev_ans = node.l.answers[-1]\n",
        "            else:\n",
        "                prev_ans = node.l.answers[-1] if node.l and node.l.answers else []\n",
        "\n",
        "\n",
        "            #     proj_ind = [ind for ind,x in enumerate(tree_nodes) if x[0]=='AGGREGATE_PROJ'][0]-1\n",
        "            # elif  op=='PROJECTION':\n",
        "            #     proj_ind = [ind for ind,x in enumerate(tree_nodes) if x[0]=='PROJECTION'][0]-1\n",
        "\n",
        "            if '!!x!!' in question: model_questions = [question.replace('!!x!!',x) for x in prev_ans ]\n",
        "            #elif '!!list!!' in question:\n",
        "            #    prev_ans = temp_ans[proj_ind]\n",
        "            #    model_questions = [question.replace('!!list!!',\", \".join(prev_ans))]\n",
        "            else: model_questions = [question]\n",
        "\n",
        "            node.questions.append(model_questions)\n",
        "            #pr= instr+few_shots+\"\"\n",
        "            #pr = [pr + inst_funct(x) for x in model_questions]\n",
        "\n",
        "            pr=construct_message_dict(instr,few_shots)\n",
        "            pr = [pr +[ construct_chat_dict(\"user\",x)] for x in model_questions]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #if verbose:    print('Prompt: ',pr)\n",
        "\n",
        "\n",
        "            ans=[]\n",
        "            if op == 'SEQ_SCAN':\n",
        "                if k in cache:\n",
        "                    ans = cache[k]\n",
        "                else:\n",
        "                    print('NOT IN CACHE')\n",
        "                    old_pr = pr[0]\n",
        "                    #if verbose:\n",
        "                    #    print('RUNNING SEQUENTIAL SCANS...')\n",
        "                    #    print(old_pr)\n",
        "\n",
        "                    #response = completion_with_backoff(model=model_arch, prompt=old_pr, temperature=0,max_tokens= max_len)\n",
        "                    #print(f\"Old PR is: {old_pr}\\n\")\n",
        "                    response = completion_with_backoff_chat(model=model_arch, messages=old_pr, temperature=0,max_tokens= max_len)\n",
        "\n",
        "                    print(f\"Completion with backoff chat response: {response}\")\n",
        "\n",
        "                    old_pr_key = json.dumps(old_pr) if isinstance(old_pr,list) else old_pr\n",
        "                    cache[old_pr_key] = response\n",
        "\n",
        "                    json.dump(cache,open(cache_fn,\"w\"),indent=2)\n",
        "\n",
        "                    #old_ans = response['choices'][0]['text']\n",
        "                    old_ans = response['choices'][0]['message']['content']\n",
        "                    if verbose:\n",
        "                        print(old_ans)\n",
        "                    temp_unfilt_ans = []\n",
        "                    temp_questions = []\n",
        "                    ans = [add_more_seq_scan(temp_unfilt_ans,model_arch,temp_questions,old_pr,old_ans,max_tries=2,increase_threshold=5)]\n",
        "\n",
        "                    node.unfiltered_answers.append(temp_unfilt_ans)\n",
        "                    node.questions.append(temp_questions)\n",
        "\n",
        "                    cache[k] = ans\n",
        "                    json.dump(cache,open(cache_fn,\"w\"),indent=2)\n",
        "\n",
        "\n",
        "            else:\n",
        "                    batch_ans = answer_batch_questions_chat(model_questions,pr,label,cache_fn,model_arch,max_len,verbose=verbose)\n",
        "                    ans.extend(batch_ans)\n",
        "\n",
        "\n",
        "            node.unfiltered_answers.append(ans)\n",
        "            if verbose:   print('Unfiltered Answer: ',ans)\n",
        "\n",
        "            if op == 'SEQ_SCAN':\n",
        "              if ans:\n",
        "                  ans = ans[0]\n",
        "                  ans = ans[:-1] if ans[-1]=='.' else ans\n",
        "                  ans = ans.replace(' and ','')\n",
        "                  ans = ans.split(',') #if ',' in ans else ans.split(' ')\n",
        "                  ans = [x for x in ans if '.' not in x] #remove 'India.Yemen'\n",
        "                  ans = list(set(ans))\n",
        "                  ans =[x for x in ans if x]\n",
        "              node.answers.append(ans)\n",
        "              if verbose:\n",
        "                  print('Final Answer: ',ans)\n",
        "\n",
        "            elif op == 'FILTER':\n",
        "                filtered_ans=[]\n",
        "                if ans:\n",
        "                    bool_index = [x.replace('.','').strip() for x in ans]\n",
        "                    bool_index = [x=='Yes' for x in bool_index]\n",
        "                    filtered_ans = list(compress(node.l.answers[-1], bool_index))\n",
        "                if verbose:\n",
        "                    print('Final Answer: ', filtered_ans)\n",
        "                node.answers.append(filtered_ans)\n",
        "                if not filtered_ans:\n",
        "                    status = 'EMPTY'\n",
        "                    if verbose:\n",
        "                        print('EMPTY')\n",
        "                    break\n",
        "\n",
        "            elif op != 'AGGREGATE_OP' and op!='AGGREGATE_count_star()':\n",
        "                  #ans = ans[0]\n",
        "                  node.answers.append(ans)\n",
        "                  if verbose:\n",
        "                      print('Final Answer: ', ans)\n",
        "            print('\\n')\n",
        "            node.status = status\n",
        "\n",
        "\n",
        "def compute_tree(node,model_arch,instr,few_shots,inst_funct,label,augmented_question_maps,verbose=False):\n",
        "    if node and node.text:\n",
        "        print(f\"Node Questions: {node.questions}\")\n",
        "        print(f\"Node Answers: {node.answers}\")\n",
        "        compute_tree(node.l,model_arch,instr,few_shots,inst_funct,label,augmented_question_maps,verbose=verbose)\n",
        "        compute_tree(node.r,model_arch,instr,few_shots,inst_funct,label,augmented_question_maps,verbose=verbose)\n",
        "        compute_node(node,model_arch,instr,few_shots,inst_funct,label,augmented_question_maps,verbose=verbose)\n",
        "\n",
        "\n",
        "\n",
        "# from QueryTree import *\n",
        "\n",
        "import time\n",
        "\n",
        "c5=None\n",
        "\n",
        "def GPT_SPW_seq(model_arch,df,instr,few_shots,inst_funct,label,augmented_question_maps,query_plan_dict,verbose=False):\n",
        "    global b\n",
        "    # create file for answers\n",
        "    json.dump([],open(label+\".json\",\"w\"),indent=3)\n",
        "\n",
        "    # check if there is cache data\n",
        "    cache_fn=label+'_cache.json'\n",
        "    mode = 'r' if cache_fn in os.listdir('.') else 'w'\n",
        "    if verbose: print('Mode: ',mode)\n",
        "    cache=json.load(open(cache_fn,'r')) if mode=='r' else dict()\n",
        "\n",
        "    print(\"Checkpoint 1: Before the FOR loop\\n\")\n",
        "\n",
        "    Questions = []\n",
        "    Answers = []\n",
        "    Unfiltered_Answers = []\n",
        "\n",
        "    for index,row in df.iterrows():\n",
        "        # get query\n",
        "        query = row.Query\n",
        "\n",
        "        # get duckdb con\n",
        "        # con = run_db(db_files[row.Database])\n",
        "        # con.execute(\"PRAGMA enable_profiling='query_tree';\")\n",
        "        # con.execute(\"PRAGMA explain_output='ALL';\")\n",
        "\n",
        "        #get logical execution plan\n",
        "        if verbose:\n",
        "            print(f\"Query: {query}\")\n",
        "            print('\\n')\n",
        "        if query in query_plan_dict:\n",
        "            root = query_plan_dict[query]\n",
        "        else:\n",
        "            try:\n",
        "                con.execute(\"EXPLAIN \"+query.replace('\"',\"'\"))\n",
        "                s = con.fetchall()[0][1].split('\\n')\n",
        "                if verbose:\n",
        "                    print(\"\\n\".join(s))\n",
        "                    print('\\n')\n",
        "                root = parse_query_tree(s)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(\"comes into the exception, so continues\")\n",
        "                continue\n",
        "        print(\"Out Of The If/Else Block\")\n",
        "        b=root\n",
        "        #tree_nodes = []\n",
        "        #get_tree_elements(root,tree_nodes)\n",
        "\n",
        "\n",
        "        #invert tree nodes\n",
        "        #tree_nodes = tree_nodes[::-1]\n",
        "\n",
        "\n",
        "\n",
        "        #separate filter\n",
        "        #tree_nodes = adjust_nodes(tree_nodes)\n",
        "        tree_adjust_nodes(root)\n",
        "\n",
        "        #if verbose:\n",
        "        #    print_tree(root)\n",
        "\n",
        "        # temp_ans=[]\n",
        "        # temp_unfilt_ans=[]\n",
        "        # temp_questions=[]\n",
        "\n",
        "        compute_tree(root,model_arch,instr,few_shots,inst_funct,label,augmented_question_maps,verbose=verbose)\n",
        "        tree_nodes,questions,answers,unfiltered_answers = get_snippet(root,[],[],[],[])\n",
        "\n",
        "        Questions += questions[0]\n",
        "        Unfiltered_Answers += unfiltered_answers[0]\n",
        "        Answers += answers[0]\n",
        "\n",
        "        snippet = {'Gold Question':row.Question,'Gold Answer':row.Answer,'Query':row.Query,\n",
        "                   'Tree Nodes':tree_nodes,'LP Questions':questions,'LP Answers':answers,\n",
        "                   'LP Unfiltered Answers':unfiltered_answers,'qqqqqqqqqqqq':root.status}\n",
        "\n",
        "\n",
        "\n",
        "        log = json.load(open(label+\".json\",\"r\"))\n",
        "        log.append(snippet)\n",
        "        json.dump(log,open(label+\".json\",\"w\"),indent=3)\n",
        "        #json.dump(cache,open(cache_fn,\"w\"),indent=2)\n",
        "        time.sleep(3)\n",
        "        print(\"===================================================================================\")\n",
        "    return Questions, Answers, Unfiltered_Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k40S-EpSJgAi"
      },
      "source": [
        "## **Selection**\n",
        "Since using the complete Spider dataset would use up our tokens, we select a few queries that are not expensive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8sVpSUu-Jfle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeefb84a-bf13-4012-ce73-a5d925f666ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    what is the average population of the us by state\n",
            "Name: Question, dtype: object\n",
            "0    [(4415590.666666667,)]\n",
            "Name: Answer, dtype: object\n",
            "0    geo\n",
            "Name: Database, dtype: object\n",
            "0    SELECT AVG ( population ) FROM state;\n",
            "Name: Query, dtype: object\n",
            "0    Spider\n",
            "Name: Dataset, dtype: object\n",
            "0    SPA\n",
            "Name: Type, dtype: object\n",
            "0    SAP\n",
            "Name: Type2, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# sel_DF is the Selected DataFrame\n",
        "sel_DF = df.iloc[12:13]\n",
        "\n",
        "#sel_indices = np.r_[40, 46:48]\n",
        "#sel_DF = df.iloc[sel_indices]\n",
        "\n",
        "#50, 42, 41, 40, 38, 37, 34, 3\n",
        "\n",
        "sel_DF = sel_DF.reset_index(drop=True)\n",
        "for column in sel_DF.columns:\n",
        "  print(sel_DF[column])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86WoKze2HagX"
      },
      "source": [
        "## **Creating DFs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6sXxLaIJHczs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "create_DF = pd.DataFrame([\n",
        "{\n",
        "'Question': 'What are the names of all the continents?',\n",
        "'Answer': ['Africa', 'Asia', 'Europe', 'North America', 'South America', 'Australia', 'Antarctica'],\n",
        "'Database': 'geo',\n",
        "'Query': 'SELECT T1.name FROM continents as T1',\n",
        "'Dataset': 'Spider',\n",
        "'Type': 'SPA',\n",
        "'Type2': 'SAP'\n",
        "},\n",
        "{\n",
        "'Question': 'List all colors of the rainbow.',\n",
        "'Answer': ['Red', 'Orange', 'Yellow', 'Green', 'Blue', 'Indigo', 'Violet'],\n",
        "'Database': 'world_1',\n",
        "'Query': 'SELECT T1.name FROM rainbow_colors as T1',\n",
        "'Dataset': 'Spider',\n",
        "'Type': 'SPA',\n",
        "'Type2': 'SAP'\n",
        "},\n",
        "{\n",
        "\"Question\": \"What are the names of all the oceans?\",\n",
        "\"Answer\": [\"Pacific Ocean\", \"Atlantic Ocean\", \"Indian Ocean\", \"Southern Ocean\", \"Arctic Ocean\"],\n",
        "\"Database\": \"geo\",\n",
        "\"Query\": \"SELECT T1.name FROM oceans as T1\",\n",
        "\"Dataset\": \"Spider\",\n",
        "\"Type\": \"SPA\",\n",
        "\"Type2\": \"SAP\"\n",
        "},\n",
        "{\n",
        "\"Question\": \"List the twelve signs of the zodiac\",\n",
        "\"Answer\": [\"Aries\", \"Taurus\", \"Gemini\", \"Cancer\", \"Leo\", \"Virgo\", \"Libra\", \"Scorpio\", \"Sagittarius\", \"Capricorn\", \"Aquarius\", \"Pisces\"],\n",
        "\"Database\": \"astrology\",\n",
        "\"Query\": \"SELECT T1.name FROM zodiac_signs as T1\",\n",
        "\"Dataset\": \"Spider\",\n",
        "\"Type\": \"SPA\",\n",
        "\"Type2\": \"SAP\"\n",
        "},\n",
        "{\n",
        "\"Question\": \"List the planets of the solar system.\",\n",
        "\"Answer\": [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"],\n",
        "\"Database\": \"space\",\n",
        "\"Query\": \"SELECT T1.name FROM planets as T1\",\n",
        "\"Dataset\": \"Spider\",\n",
        "\"Type\": \"SPA\",\n",
        "\"Type2\": \"SAP\"\n",
        "},\n",
        "{\n",
        "\"Question\": \"List the twelve dwarfs from 'Snow White'.\",\n",
        "\"Answer\": [\"Doc\", \"Grumpy\", \"Happy\", \"Sleepy\", \"Bashful\", \"Sneezy\", \"Dopey\", \"Humbert\", \"Grimm\", \"Bossy\", \"Biggy\", \"Smoky\"],\n",
        "\"Database\": \"characters\",\n",
        "\"Query\": \"SELECT T1.name FROM snow_white_dwarfs as T1\",\n",
        "\"Dataset\": \"Spider\",\n",
        "\"Type\": \"SPA\",\n",
        "\"Type2\": \"SAP\"\n",
        "},\n",
        "# {\n",
        "# \"Question\": \"Name the five permanent members of the United Nations Security Council.\",\n",
        "# \"Answer\": [\"China\", \"France\", \"Russia\", \"United Kingdom\", \"United States\"],\n",
        "# \"Database\": \"UN\",\n",
        "# \"Query\": \"SELECT T1.name FROM security_council as T1 WHERE T1.permanent_member = True\",\n",
        "# \"Dataset\": \"Spider\",\n",
        "# \"Type\": \"SPA\",\n",
        "# \"Type2\": \"SAP\"\n",
        "# },\n",
        "{\n",
        "\"Question\": \"List the seven deadly sins.\",\n",
        "\"Answer\": [\"Lust\", \"Gluttony\", \"Greed\", \"Sloth\", \"Wrath\", \"Envy\", \"Pride\"],\n",
        "\"Database\": \"sins\",\n",
        "\"Query\": \"SELECT T1.name FROM deadly_sins as T1\",\n",
        "\"Dataset\": \"Spider\",\n",
        "\"Type\": \"SPA\",\n",
        "\"Type2\": \"SAP\"\n",
        "},\n",
        "{\n",
        "\"Question\": \"Name five Nobel Prize categories.\",\n",
        "\"Answer\": [\"Physics\", \"Chemistry\", \"Medicine\", \"Literature\", \"Peace\"],\n",
        "\"Database\": \"nobel_prizes\",\n",
        "\"Query\": \"SELECT T1.name FROM nobel_prize_categories as T1\",\n",
        "\"Dataset\": \"Spider\",\n",
        "\"Type\": \"SPA\",\n",
        "\"Type2\": \"SAP\"\n",
        "},\n",
        "{\n",
        "\"Question\": \"Name all seven books in the Harry Potter series.\",\n",
        "\"Answer\": [\"Harry Potter and the Philosopher's Stone\", \"Harry Potter and the Chamber of Secrets\", \"Harry Potter and the Prisoner of Azkaban\", \"Harry Potter and the Goblet of Fire\", \"Harry Potter and the Order of the Phoenix\", \"Harry Potter and the Half-Blood Prince\", \"Harry Potter and the Deathly Hallows\"],\n",
        "\"Database\": \"harry_potter\",\n",
        "\"Query\": \"SELECT T1.name FROM harry_potter_titles as T1\",\n",
        "\"Dataset\": \"Spider\",\n",
        "\"Type\": \"SPA\",\n",
        "\"Type2\": \"SAP\"\n",
        "}\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4gVPVP8jpIr"
      },
      "source": [
        "# **TREE BUILDING OF THE 50 QUERIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SjxVY5njjuSH"
      },
      "outputs": [],
      "source": [
        "# tree_nodes = []\n",
        "\n",
        "# db_files = {'geo':'data/geography-db.added-in-2020.sqlite',\n",
        "#             'world_1':'spider/database/world_1/world_1.sqlite',\n",
        "#             'flight_4':'spider/database/flight_4/flight_4.sqlite',\n",
        "#             'flight_2':'spider/database/flight_2/flight_2.sqlite',\n",
        "#             'singer':'spider/database/singer/singer.sqlite'\n",
        "#             }\n",
        "\n",
        "# for i, (db, q) in enumerate(zip(create_DF.Database, create_DF.Query)):\n",
        "#     print(i)\n",
        "#     print(create_DF.Question[i])\n",
        "#     intermediate_tree_nodes = []\n",
        "#     print(q)\n",
        "\n",
        "#     con = run_db(db_files[db])\n",
        "#     con.execute(\"PRAGMA enable_profiling='query_tree';\")\n",
        "#     con.execute(\"PRAGMA explain_output='ALL';\")\n",
        "#     con.execute(\"EXPLAIN \" + q.replace('\"', \"'\"))\n",
        "#     s = con.fetchall()[0][1]\n",
        "#     print(s)\n",
        "\n",
        "#     try:\n",
        "#         root = parse_query_tree(s.split('\\n'))\n",
        "#         get_tree_elements(root, intermediate_tree_nodes)\n",
        "#         tree_nodes.append(intermediate_tree_nodes[::-1])\n",
        "#         print('PARSED')\n",
        "#     except:\n",
        "#         print('NOT PARSED')\n",
        "\n",
        "#     print('=======================================')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Crxqd3plr_f2"
      },
      "outputs": [],
      "source": [
        "join_query_trees={}\n",
        "tree_nodes = []\n",
        "\n",
        "\n",
        "# EXTRA\n",
        "# How many continents exist?\n",
        "q = 'SELECT T1.name FROM continents as T1'\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','continents']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['continents']\n",
        "\n",
        "# c2 = Node()\n",
        "# c2.text = ['PROJECTION','official_name']\n",
        "# c2.op = 'PROJECTION'\n",
        "# c2.args=['Official Name']\n",
        "# c2.l = c1\n",
        "\n",
        "join_query_trees[q] = c1\n",
        "\n",
        "# List all colors of the rainbow\n",
        "q = 'SELECT T1.name FROM rainbow_colors as T1'\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','rainbow_colors']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['rainbow_colors']\n",
        "\n",
        "# c2 = Node()\n",
        "# c2.text = ['PROJECTION','name']\n",
        "# c2.op = 'PROJECTION'\n",
        "# c2.args=['Name']\n",
        "# c2.l = c1\n",
        "\n",
        "join_query_trees[q] = c1\n",
        "\n",
        "\n",
        "# What are the names of all the oceans?\n",
        "q = 'SELECT T1.name FROM oceans as T1'\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','oceans']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['oceans']\n",
        "\n",
        "# c2 = Node()\n",
        "# c2.text = ['PROJECTION','name']\n",
        "# c2.op = 'PROJECTION'\n",
        "# c2.args=['Name']\n",
        "# c2.l = c1\n",
        "\n",
        "join_query_trees[q] = c1\n",
        "\n",
        "#\n",
        "q = 'SELECT T1.name FROM zodiac_signs as T1'\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','zodiac_signs']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['zodiac_signs']\n",
        "\n",
        "# c2 = Node()\n",
        "# c2.text = ['PROJECTION','name']\n",
        "# c2.op = 'PROJECTION'\n",
        "# c2.args=['Name']\n",
        "# c2.l = c1\n",
        "\n",
        "join_query_trees[q] = c1\n",
        "\n",
        "#\n",
        "q = 'SELECT T1.name FROM planets as T1'\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','planets']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['zodiac_signs']\n",
        "\n",
        "# c2 = Node()\n",
        "# c2.text = ['PROJECTION','name']\n",
        "# c2.op = 'PROJECTION'\n",
        "# c2.args=['Name']\n",
        "# c2.l = c1\n",
        "\n",
        "join_query_trees[q] = c1\n",
        "\n",
        "#\n",
        "q = 'SELECT T1.name FROM snow_white_dwarfs as T1'\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','snow_white_dwarfs']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['snow_white_dwarfs']\n",
        "\n",
        "# c2 = Node()\n",
        "# c2.text = ['PROJECTION','name']\n",
        "# c2.op = 'PROJECTION'\n",
        "# c2.args=['Name']\n",
        "# c2.l = c1\n",
        "\n",
        "join_query_trees[q] = c1\n",
        "\n",
        "#\n",
        "q = 'SELECT T1.name FROM deadly_sins as T1'\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','deadly_sins']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['deadly_sins']\n",
        "\n",
        "# c2 = Node()\n",
        "# c2.text = ['PROJECTION','name']\n",
        "# c2.op = 'PROJECTION'\n",
        "# c2.args=['Name']\n",
        "# c2.l = c1\n",
        "\n",
        "join_query_trees[q] = c1\n",
        "\n",
        "#\n",
        "q = 'SELECT T1.name FROM nobel_prize_categories as T1'\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','nobel_prize_categories']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['nobel_prize_categories']\n",
        "\n",
        "# c2 = Node()\n",
        "# c2.text = ['PROJECTION','name']\n",
        "# c2.op = 'PROJECTION'\n",
        "# c2.args=['Name']\n",
        "# c2.l = c1\n",
        "\n",
        "join_query_trees[q] = c1\n",
        "\n",
        "#\n",
        "q = 'SELECT T1.name FROM harry_potter_titles as T1'\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','harry_potter_titles']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['harry_potter_titles']\n",
        "\n",
        "# c2 = Node()\n",
        "# c2.text = ['PROJECTION','name']\n",
        "# c2.op = 'PROJECTION'\n",
        "# c2.args=['Name']\n",
        "# c2.l = c1\n",
        "\n",
        "join_query_trees[q] = c1\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "# 40\n",
        "# Which language is the most popular in Aruba?\n",
        "q = 'SELECT T2.Language FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T1.Name = \"Aruba\" ORDER BY Percentage DESC LIMIT 1'\n",
        "# ┌───────────────────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           #[0.0]          │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           LIMIT           │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │          ORDER_BY         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          ORDERS:          │\n",
        "# │           #[0.1]          │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          Language         │\n",
        "# │         Percentage        │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │      (Name = 'Aruba')     │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           INNER           ├──────────────┐\n",
        "# │    (Code = CountryCode)   │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││          SEQ_SCAN         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          country          ││      countrylanguage      │\n",
        "# └───────────────────────────┘└───────────────────────────┘\n",
        "\n",
        "# NOT PARSED\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','country']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['country']\n",
        "\n",
        "c2 = Node()\n",
        "c2.text = ['SEQ_SCAN','countrylanguage']\n",
        "c2.op = 'SEQ_SCAN'\n",
        "c2.args=['countrylanguage']\n",
        "\n",
        "c3 = Node()\n",
        "c3.text = ['JOIN','city']\n",
        "c3.op = 'JOIN'\n",
        "c3.key_left='What is the country code of !!x!!?. Answer briefly.'\n",
        "c3.key_right = 'What is the country code of the country that speaks !!x!!? Answer briefly.'\n",
        "c3.filter_key='left'\n",
        "\n",
        "c4 = Node()\n",
        "c4.text = ['FILTER',\"(Name = 'Aruba')\"]\n",
        "c4.op = 'FILTER'\n",
        "c4.args=[\"(Name = 'Aruba')\"]\n",
        "\n",
        "c5 = Node()\n",
        "c5.text = ['PROJECTION','Language','Percentage']\n",
        "c5.op = 'PROJECTION'\n",
        "c5.args=['Language','Percentage']\n",
        "\n",
        "\n",
        "c6 = Node()\n",
        "c6.text = ['ORDER_BY','Percentage']\n",
        "c6.op = 'ORDER_BY'\n",
        "c6.args=['Percentage']\n",
        "\n",
        "\n",
        "c7 = Node()\n",
        "c7.text = ['PROJECTION','Language']\n",
        "c7.op = 'PROJECTION'\n",
        "c7.args=['Language']\n",
        "\n",
        "c3.l=c1\n",
        "c3.r = c2\n",
        "c4.l=c3\n",
        "c5.l=c4\n",
        "c6.l=c5\n",
        "c7.l=c6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 41\n",
        "# what is the capital of states that have cities named durham\n",
        "q = 'SELECT t2.capital FROM state AS t2 JOIN city AS t1 ON t2.state_name = t1.state_name WHERE t1.city_name = \"durham\";'\n",
        "# ┌───────────────────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          capital          │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │   (city_name = 'durham')  │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           INNER           ├──────────────┐\n",
        "# │ (state_name = state_name) │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││          SEQ_SCAN         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           state           ││            city           │\n",
        "# └───────────────────────────┘└───────────────────────────┘\n",
        "\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','state']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['state']\n",
        "\n",
        "c2 = Node()\n",
        "c2.text = ['SEQ_SCAN','city']\n",
        "c2.op = 'SEQ_SCAN'\n",
        "c2.args=['city']\n",
        "\n",
        "c3 = Node()\n",
        "c3.text = ['JOIN','city']\n",
        "c3.op = 'JOIN'\n",
        "c3.key_left='What is the state name of !!x!!? Answer briefly.'\n",
        "c3.key_right = 'What is the state name of !!x!!? Answer briefly.'\n",
        "c3.filter_key='right'\n",
        "\n",
        "c4 = Node()\n",
        "c4.text = ['FILTER',\"(city_name = 'durham')\"]\n",
        "c4.op = 'FILTER'\n",
        "c4.args=[\"(city_name = 'durham')\"]\n",
        "c4.filled_question = 'Is !!x!! the same as Durham?'\n",
        "c5 = Node()\n",
        "c5.text = ['PROJECTION','capital']\n",
        "c5.op = 'PROJECTION'\n",
        "c5.args=['capital']\n",
        "c5.filled_question = 'What is the capital of state of !!x!!?'\n",
        "\n",
        "\n",
        "c3.l=c1\n",
        "c3.r = c2\n",
        "c4.l=c3\n",
        "c5.l=c4\n",
        "\n",
        "join_query_trees[q] = c5\n",
        "\n",
        "\n",
        "\n",
        "# NOT PARSED\n",
        "# =======================================\n",
        "# 42\n",
        "# What are the regions that use English or Dutch?\n",
        "q = 'SELECT DISTINCT T1.Region FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = \"English\" OR T2.Language = \"Dutch\"'\n",
        "# ┌───────────────────────────┐\n",
        "# │          DISTINCT         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │       \"T1\".\"Region\"       │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           Region          │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │((Language = 'English') OR │\n",
        "# │   (Language = 'Dutch'))   │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           INNER           ├──────────────┐\n",
        "# │    (Code = CountryCode)   │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││          SEQ_SCAN         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          country          ││      countrylanguage      │\n",
        "# └───────────────────────────┘└───────────────────────────┘\n",
        "\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','country']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['country']\n",
        "\n",
        "c2 = Node()\n",
        "c2.text = ['SEQ_SCAN','countrylanguage']\n",
        "c2.op = 'SEQ_SCAN'\n",
        "c2.args=['countrylanguage']\n",
        "\n",
        "c3 = Node()\n",
        "c3.text = ['JOIN','city']\n",
        "c3.op = 'JOIN'\n",
        "c3.key_left='What is the country code of !!x!!? Answer briefly.'\n",
        "c3.key_right = 'What is the country code of the country that speaks !!x!!? Answer briefly.'\n",
        "c3.filter_key='right'\n",
        "\n",
        "c4 = Node()\n",
        "c4.text = ['FILTER',\"(Language = 'English')\",\"(Language = 'Dutch')\"]\n",
        "c4.op = 'FILTER'\n",
        "c4.args=[\"(Language = 'English')\",\"(Language = 'Dutch')\"]\n",
        "\n",
        "c5 = Node()\n",
        "c5.text = ['PROJECTION','Region']\n",
        "c5.op = 'PROJECTION'\n",
        "c5.args=['Region']\n",
        "c5.filled_question = 'What is the region that speaks !!x!!?'\n",
        "\n",
        "\n",
        "c3.l=c1\n",
        "c3.r = c2\n",
        "c4.l=c3\n",
        "c5.l=c4\n",
        "\n",
        "join_query_trees[q] = c5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# NOT PARSED\n",
        "# =======================================\n",
        "# 43\n",
        "# What is the official language spoken in the country whose head of state is Beatrix?\n",
        "q = 'SELECT T2.Language FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T1.HeadOfState = \"Beatrix\" AND T2.IsOfficial = \"T\"'\n",
        "# ┌───────────────────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          Language         │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │ (HeadOfState = 'Beatrix') │\n",
        "# │ (IsOfficial = CAST('T' AS │\n",
        "# │          BOOLEAN))        │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           INNER           ├──────────────┐\n",
        "# │    (Code = CountryCode)   │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││          SEQ_SCAN         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          country          ││      countrylanguage      │\n",
        "# └───────────────────────────┘└───────────────────────────┘\n",
        "\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','country']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['country']\n",
        "\n",
        "c2 = Node()\n",
        "c2.text = ['SEQ_SCAN','countrylanguage']\n",
        "c2.op = 'SEQ_SCAN'\n",
        "c2.args=['countrylanguage']\n",
        "\n",
        "c3 = Node()\n",
        "c3.text = ['JOIN','city']\n",
        "c3.op = 'JOIN'\n",
        "c3.key_left='What is the country code of !!x!!? Answer briefly.'\n",
        "c3.key_right = 'What is the country code of the country that speaks !!x!!? Answer briefly.'\n",
        "c3.filter_key='left'\n",
        "\n",
        "\n",
        "c4 = Node()\n",
        "c4.text = ['FILTER',\"(HeadOfState = 'Beatrix')\",\"(IsOfficial = CAST('T' AS BOOLEAN))\"]\n",
        "c4.op = 'FILTER'\n",
        "c4.args=[\"(HeadOfState = 'Beatrix')\",\"(IsOfficial = CAST('T' AS BOOLEAN))\"]\n",
        "\n",
        "c5 = Node()\n",
        "c5.text = ['PROJECTION','Language']\n",
        "c5.op = 'PROJECTION'\n",
        "c5.args=['Language']\n",
        "c5.filled_question = 'What is the language of !!x!!?'\n",
        "\n",
        "\n",
        "\n",
        "c3.l=c1\n",
        "c3.r = c2\n",
        "c4.l=c3\n",
        "c5.l=c4\n",
        "\n",
        "join_query_trees[q]=c5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# NOT PARSED\n",
        "# =======================================\n",
        "# 44\n",
        "# what state has no rivers\n",
        "# SELECT state_name FROM state WHERE state_name NOT IN ( SELECT traverse FROM river );\n",
        "# ┌───────────────────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │         state_name        │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │       (NOT SUBQUERY)      │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │            MARK           ├──────────────┐\n",
        "# │   (state_name = #[8.0])   │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           state           ││          traverse         │\n",
        "# └───────────────────────────┘└─────────────┬─────────────┘\n",
        "#                              ┌─────────────┴─────────────┐\n",
        "#                              │          SEQ_SCAN         │\n",
        "#                              │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "#                              │           river           │\n",
        "#                              └───────────────────────────┘\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# NOT PARSED\n",
        "# =======================================\n",
        "# 45\n",
        "# how many rivers do not traverse the state with the capital albany\n",
        "# SELECT COUNT ( river_name ) FROM river WHERE traverse NOT IN ( SELECT state_name FROM state WHERE capital = \"albany\" );\n",
        "# ┌───────────────────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │     count(river_name)     │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │         AGGREGATE         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │     count(river_name)     │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │       (NOT SUBQUERY)      │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │            MARK           ├──────────────┐\n",
        "# │    (traverse = #[8.0])    │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           river           ││         state_name        │\n",
        "# └───────────────────────────┘└─────────────┬─────────────┘\n",
        "#                              ┌─────────────┴─────────────┐\n",
        "#                              │           FILTER          │\n",
        "#                              │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "#                              │    (capital = 'albany')   │\n",
        "#                              └─────────────┬─────────────┘\n",
        "#                              ┌─────────────┴─────────────┐\n",
        "#                              │          SEQ_SCAN         │\n",
        "#                              │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "#                              │           state           │\n",
        "#                              └───────────────────────────┘\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# NOT PARSED\n",
        "# =======================================\n",
        "# 46\n",
        "# What is the number of distinct continents where Chinese is spoken?\n",
        "q ='SELECT COUNT( DISTINCT Continent) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = \"Chinese\"'\n",
        "# ┌───────────────────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │ count(DISTINCT Continent) │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │         AGGREGATE         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │ count(DISTINCT Continent) │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │   (Language = 'Chinese')  │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           INNER           ├──────────────┐\n",
        "# │    (Code = CountryCode)   │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││          SEQ_SCAN         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          country          ││      countrylanguage      │\n",
        "# └───────────────────────────┘└───────────────────────────┘\n",
        "\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','country']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['country']\n",
        "\n",
        "c2 = Node()\n",
        "c2.text = ['SEQ_SCAN','countrylanguage']\n",
        "c2.op = 'SEQ_SCAN'\n",
        "c2.args=['countrylanguage']\n",
        "\n",
        "c3 = Node()\n",
        "c3.text = ['JOIN','city']\n",
        "c3.op = 'JOIN'\n",
        "c3.key_left='What is the country code of !!x!!? Answer briefly.'\n",
        "c3.key_right = 'What is the country code of the country that speaks !!x!!? Answer briefly.'\n",
        "c3.filter_key='right'\n",
        "\n",
        "c4 = Node()\n",
        "c4.text = ['FILTER',\"(Language = 'Chinese')\"]\n",
        "c4.op = 'FILTER'\n",
        "c4.args=[\"(Language = 'Chinese')\"]\n",
        "c4.filled_question = 'Is !!x!! the same as Chinese?'\n",
        "\n",
        "c5 = Node()\n",
        "c5.text = ['AGGREGATE','count(Continent)']\n",
        "c5.op = 'AGGREGATE'\n",
        "c5.args=['count(Continent)']\n",
        "c5.filled_question = 'What is the continent that speaks !!x!!?'\n",
        "\n",
        "# c6 = Node()\n",
        "# c6.text = ['PROJECTION','count(Continent)']\n",
        "# c6.op = 'PROJECTION'\n",
        "# c6.args=['count(Continent)']\n",
        "\n",
        "c3.l=c1\n",
        "c3.r = c2\n",
        "c4.l=c3\n",
        "c5.l=c4\n",
        "#c6.l=c5\n",
        "\n",
        "join_query_trees[q] = c5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# NOT PARSED\n",
        "# =======================================\n",
        "# 47\n",
        "# Which region is the city Kabul located in?\n",
        "q = 'SELECT Region FROM country AS T1 JOIN city AS T2 ON T1.Code = T2.CountryCode WHERE T2.Name = \"Kabul\"'\n",
        "# ┌───────────────────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           Region          │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │      (Name = 'Kabul')     │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           INNER           ├──────────────┐\n",
        "# │    (Code = CountryCode)   │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││          SEQ_SCAN         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          country          ││            city           │\n",
        "# └───────────────────────────┘└───────────────────────────┘\n",
        "\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','country']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['country']\n",
        "\n",
        "c2 = Node()\n",
        "c2.text = ['SEQ_SCAN','city']\n",
        "c2.op = 'SEQ_SCAN'\n",
        "c2.args=['city']\n",
        "\n",
        "c3 = Node()\n",
        "c3.text = ['JOIN','city']\n",
        "c3.op = 'JOIN'\n",
        "c3.key_left='What is the country code of !!x!!? Answer briefly.'\n",
        "c3.key_right = 'What is the country code of the country of !!x!!? Answer briefly.'\n",
        "c3.filter_key='right'\n",
        "\n",
        "c4 = Node()\n",
        "c4.text = ['FILTER',\"(Name = 'Kabul')\"]\n",
        "c4.op = 'FILTER'\n",
        "c4.args=[\"(Name = 'Kabul')\"]\n",
        "c4.filled_question = 'Is !!x!! the same as Kabul?'\n",
        "c5 = Node()\n",
        "c5.text = ['PROJECTION','Region']\n",
        "c5.op = 'PROJECTION'\n",
        "c5.args=['Region']\n",
        "c5.filled_question = 'What is the region of !!x!!?'\n",
        "\n",
        "c3.l=c1\n",
        "c3.r = c2\n",
        "c4.l=c3\n",
        "c5.l=c4\n",
        "\n",
        "\n",
        "join_query_trees[q] = c5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# NOT PARSED\n",
        "# =======================================\n",
        "# 48\n",
        "# How many official languages does Afghanistan have?\n",
        "q = 'SELECT COUNT(*) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T1.Name = \"Afghanistan\" AND IsOfficial = \"T\"'\n",
        "# ┌───────────────────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │        count_star()       │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │         AGGREGATE         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │        count_star()       │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │   (Name = 'Afghanistan')  │\n",
        "# │ (IsOfficial = CAST('T' AS │\n",
        "# │          BOOLEAN))        │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           INNER           ├──────────────┐\n",
        "# │    (Code = CountryCode)   │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││          SEQ_SCAN         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          country          ││      countrylanguage      │\n",
        "# └───────────────────────────┘└───────────────────────────┘\n",
        "\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','country']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['country']\n",
        "\n",
        "c2 = Node()\n",
        "c2.text = ['SEQ_SCAN','countrylanguage']\n",
        "c2.op = 'SEQ_SCAN'\n",
        "c2.args=['countrylanguage']\n",
        "\n",
        "c3 = Node()\n",
        "c3.text = ['JOIN','city']\n",
        "c3.op = 'JOIN'\n",
        "c3.key_left='What is the country code of !!x!!? Answer briefly.'\n",
        "c3.key_right = 'What is the country code of the country that speaks !!x!!? Answer briefly.'\n",
        "c3.filter_key='left'\n",
        "\n",
        "c4 = Node()\n",
        "c4.text = ['FILTER',\"(Name = 'Afghanistan')\",\"(IsOfficial = CAST('T' AS BOOLEAN))\"]\n",
        "c4.op = 'FILTER'\n",
        "c4.args=[\"(Name = 'Afghanistan')\",\"(IsOfficial = CAST('T' AS BOOLEAN))\"]\n",
        "\n",
        "c5 = Node()\n",
        "c5.text = ['AGGREGATE','count_star()']\n",
        "c5.op = 'AGGREGATE'\n",
        "c5.args=['count_star()']\n",
        "\n",
        "# c6 = Node()\n",
        "# c6.text = ['PROJECTION','count_star()']\n",
        "# c6.op = 'PROJECTION'\n",
        "# c6.args=['count_star()']\n",
        "\n",
        "c3.l=c1\n",
        "c3.r = c2\n",
        "c4.l=c3\n",
        "c5.l=c4\n",
        "#c6.l=c5\n",
        "\n",
        "join_query_trees[q] = c5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# NOT PARSED\n",
        "# =======================================\n",
        "# 49\n",
        "# which capitals are not major cities\n",
        "q = 'SELECT t2.capital FROM state AS t2 JOIN city AS t1 ON t2.capital = t1.city_name WHERE t1.population <= 150000;'\n",
        "# ┌───────────────────────────┐\n",
        "# │         PROJECTION        │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │          capital          │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │           FILTER          │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │ (population <= CAST(150000│\n",
        "# │         AS BIGINT))       │\n",
        "# └─────────────┬─────────────┘\n",
        "# ┌─────────────┴─────────────┐\n",
        "# │      COMPARISON_JOIN      │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           INNER           ├──────────────┐\n",
        "# │   (capital = city_name)   │              │\n",
        "# └─────────────┬─────────────┘              │\n",
        "# ┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n",
        "# │          SEQ_SCAN         ││          SEQ_SCAN         │\n",
        "# │   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ││   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n",
        "# │           state           ││            city           │\n",
        "# └───────────────────────────┘└───────────────────────────┘\n",
        "\n",
        "c1 = Node()\n",
        "c1.text = ['SEQ_SCAN','state']\n",
        "c1.op = 'SEQ_SCAN'\n",
        "c1.args=['state']\n",
        "\n",
        "c2 = Node()\n",
        "c2.text = ['SEQ_SCAN','city']\n",
        "c2.op = 'SEQ_SCAN'\n",
        "c2.args=['city']\n",
        "\n",
        "c3 = Node()\n",
        "c3.text = ['JOIN','city']\n",
        "c3.op = 'JOIN'\n",
        "c3.key_left='What is the capital of !!x!!? Answer briefly.'\n",
        "c3.key_right = 'What is the name of !!x!!? Answer briefly.'\n",
        "c3.filter_key='right'\n",
        "\n",
        "c4 = Node()\n",
        "c4.text = ['FILTER',\"(population <= CAST(150000 AS BIGINT))\"]\n",
        "c4.op = 'FILTER'\n",
        "c4.args=[\"(population <= CAST(150000 AS BIGINT))\"]\n",
        "\n",
        "c5 = Node()\n",
        "c5.text = ['PROJECTION','capital']\n",
        "c5.op = 'PROJECTION'\n",
        "c5.args=['capital']\n",
        "c5.filled_question  = 'What is the capital of the state of !!x!!?'\n",
        "\n",
        "\n",
        "c3.l=c1\n",
        "c3.r = c2\n",
        "c4.l=c3\n",
        "c5.l=c4\n",
        "\n",
        "join_query_trees[q] = c5\n",
        "\n",
        "\n",
        "# NOT PARSED\n",
        "# =======================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xFV1gjgkud5J"
      },
      "outputs": [],
      "source": [
        "\n",
        "question_maps = {'SEQ_SCAN_state': 'List some american states.',\n",
        " \"FILTER_(state_name = 'new mexico')\": 'Is !!x!! the same as \"New Mexico\"?',\n",
        " 'PROJECTION_area': 'What is the area of !!x!!?',\n",
        " \"FILTER_(state_name = 'california')\": 'Is !!x!! the same as California?',\n",
        " 'PROJECTION_population': 'What is the population of !!x!!?',\n",
        " 'SEQ_SCAN_city': 'List some cities.',\n",
        " \"FILTER_(state_name = 'texas')\": 'Is !!x!! the same as Texas?',\n",
        " 'PROJECTION_city_name': 'What is the name of !!x!!?',\n",
        " \"FILTER_(capital = 'albany')\": 'Is Albany the capital of !!x!!?',\n",
        " 'SEQ_SCAN_lake': 'List some american lakes.',\n",
        " 'PROJECTION_lake_name': 'What is the name of the lake !!x!!?',\n",
        " 'FILTER_(area > CAST(750 AS DOUBLE) )': 'Is the area of !!x!! greater than 750 km2?',\n",
        " \"FILTER_(state_name = 'michigan')\": 'Is !!x!! in Michigan?',\n",
        " 'FILTER_(lowest_elevation = 0)': 'Is the lowest elevation in !!x!! 0 meters?',\n",
        " 'PROJECTION_highest_point': 'What is the highest point of !!x!!?',\n",
        " 'PROJECTION_state_name': 'What is the name of the state of !!x!!?',\n",
        " 'SEQ_SCAN_river': 'List some rivers in the US.',\n",
        " \"FILTER_(traverse = 'idaho')\": 'Does !!x!! traverse Idaho?',\n",
        " 'AGGREGATE_PROJ_river_name': 'What is the name of the river !!x!!?',\n",
        " \"FILTER_(traverse = 'illinois')\": 'Does !!x!! traverse Illinois?',\n",
        " 'PROJECTION_river_name': 'WHat is the name of !!x!!?',\n",
        " \"FILTER_(city_name = 'springfield')\": 'Is !!x!! the same as Springfield?',\n",
        " \"FILTER_(city_name = 'boulder')\": 'Is !!x!! the same as Boulder?',\n",
        " \"FILTER_(state_name = 'delaware')\": 'Is !!x!! in Delaware?',\n",
        " 'PROJECTION_highest_elevation': 'What is the highest elevation of !!x!!?',\n",
        " \"FILTER_(highest_point = 'guadalupe peak')\": 'Is guadalupe peak the highest point of !!x!!?',\n",
        " 'AGGREGATE_PROJ_highest_elevation': 'What is the highest elevation of !!x!!?',\n",
        " \"FILTER_(river_name = 'rio grande')\": 'Is !!x!! the same as \"Rio Grande\"?',\n",
        " 'PROJECTION_length': 'What is the length of !!x!!?',\n",
        " \"FILTER_(state_name = 'rhode island ')\": 'Is !!x!! the same as \"rhode island\"?',\n",
        " 'AGGREGATE_PROJ_capital': 'What is the capital of !!x!!?',\n",
        " 'AGGREGATE_PROJ_city_name': 'What is the name of !!x!!?',\n",
        " 'FILTER_(population > 150000)': 'Does !!x!! have a population greater than 150000?',\n",
        " \"FILTER_(river_name = 'colorado')\": 'Is !!x!! the same as Colorado?',\n",
        " \"FILTER_(city_name = 'seattle')\": 'Is !!x!! the same as Seattle?',\n",
        " \"FILTER_(state_name = 'washington')\": 'Is !!x!! in Washington?',\n",
        " 'AGGREGATE_PROJ_population': 'What is the popualtion of !!x!!?',\n",
        " 'AGGREGATE_PROJ_state_name': 'What is the name of the state of !!x!!?',\n",
        " 'FILTER_(length > 750)': 'Is the length of !!x!! greater than 750 miles?',\n",
        " 'PROJECTION_capital': 'What is the capital of !!x!!?',\n",
        " \"FILTER_(state_name = 'kansas')\": 'Is !!x!! in the state of Kanses?',\n",
        " \"FILTER_(state_name = 'wisconsin')\": 'Is !!x!! in the state of Wisconsin?',\n",
        " 'AGGREGATE_PROJ_area': 'What is the area of !!x!!?',\n",
        " \"FILTER_(state_name = 'wyoming')\": 'Is !!x!! the same as Wyoming?',\n",
        " 'PROJECTION_density': 'What is the density of !!x!!?',\n",
        " 'PROJECTION_lowest_point': 'What is the lowest point of !!x!!?',\n",
        " 'AGGREGATE_PROJ_length': 'What is the length of !!x!!?',\n",
        " 'PROJECTION_traverse': 'What states does !!x!! traverse?',\n",
        " 'SEQ_SCAN_mountain': 'List some mountains in the US.',\n",
        " \"FILTER_(mountain_name = 'whitney')\": 'Is !!x!! the same as \"Whitney\"?',\n",
        " \"FILTER_(city_name = 'austin')\": 'Is !!x!! the same as Austin?',\n",
        " \"FILTER_(capital = 'salem')\": 'Is Salem the capital of !!x!!?',\n",
        " \"FILTER_(river_name = 'missouri')\": 'Is !!x!! the same as \"missouri\"?',\n",
        " 'AGGREGATE_PROJ_traverse': 'What are the states that !!x!! traverses?',\n",
        " \"FILTER_(state_name = 'pennsylvania ')\": 'Is !!x!! the same as pennsylvania? ',\n",
        " 'PROJECTION_(CAST(population AS DOUBLE) / area)': 'What is the population of !!x!! divided by its area?',\n",
        " \"FILTER_(traverse = 'ohio')\": 'Does !!x!! traverse Ohio?',\n",
        " 'AGGREGATE_PROJ_DISTINCT traverse': 'What are the states that !!x!! traverses?',\n",
        " \"FILTER_(mountain_name = 'mckinley' )\": 'Is !!x!! the same as Mckinley?',\n",
        " 'PROJECTION_mountain_altitude': 'What is the altitude of !!x!!?',\n",
        " \"FILTER_(state_name = 'alaska')\": 'Is !!x!! in Alaska?',\n",
        " 'PROJECTION_mountain_name': 'What is the name of !!x!!?',\n",
        " \"FILTER_(state_name = 'massachusetts')\": 'Is !!x!! the same as massachusetts?',\n",
        " 'PROJECTION_country_name': 'What is the name of the country of !!x!!?',\n",
        " \"FILTER_(capital = 'austin')\": 'Is Austin the capital of !!x!!?',\n",
        " 'SEQ_SCAN_country': 'List some countries.',\n",
        " 'FILTER_(IndepYear > 1950)': 'Is the independence year of !!x!! greater than 1950?',\n",
        " 'PROJECTION_Name': 'What is the name of !!x!!?',\n",
        " \"FILTER_(GovernmentForm = 'Republic ')\": 'Is the government form of !!x!! republic?',\n",
        " 'AGGREGATE_count_star()': 'What is the count of the following list: !!list!!?',\n",
        " \"FILTER_(Region = 'Caribbean')\": 'Is !!x!! in the Caribbean?',\n",
        " 'AGGREGATE_PROJ_SurfaceArea': 'What is the surface area of !!x!!?',\n",
        " \"FILTER_(Name = 'Anguilla')\": 'Is !!x!! Anguilla?',\n",
        " 'PROJECTION_Continent': 'What is the continent of !!x!!?',\n",
        " \"FILTER_(Name = 'Brazil')\": 'Is !!x!! the same as Brazil?',\n",
        " 'PROJECTION_Population': 'What is the population of !!x!!?',\n",
        " 'PROJECTION_LifeExpectancy': 'What is the life expectancy of !!x!!?',\n",
        " \"FILTER_(Name = 'Angola')\": 'Is !!x!! Angola?',\n",
        " 'PROJECTION_Region': 'What is the region of !!x!!?',\n",
        " \"FILTER_(Region = 'Central Africa')\": 'Is !!x!! in central Africa?',\n",
        " 'AGGREGATE_PROJ_LifeExpectancy': 'What is the life expectancy of !!x!!?',\n",
        " \"FILTER_(Continent = 'Asia')\": 'Is !!x!! in Asia?',\n",
        " 'AGGREGATE_PROJ_Population': 'What is the population of !!x!!?',\n",
        " 'AGGREGATE_PROJ_GNP': 'What is the GNP of !!x!!?',\n",
        " \"FILTER_(Continent = 'Africa')\": 'Is !!x!! in Africa?',\n",
        " \"FILTER_(District = 'Gelderland')\": 'Is !!x!! in Gelderland district?',\n",
        " \"FILTER_(GovernmentForm = 'US Territory')\": 'Does !!x!! have US Territory as a form of government?',\n",
        " 'SEQ_SCAN_countrylanguage': 'List some languages.',\n",
        " 'AGGREGATE_PROJ_DISTINCT Language': 'What is the language of !!x!!?',\n",
        " 'AGGREGATE_PROJ_DISTINCT GovernmentForm': 'What is the government form of !!x!!?',\n",
        " \"FILTER_(Language != 'English')\": 'Is !!x!! not English?',\n",
        " 'PROJECTION_CountryCode': 'What is the alpha-3 country code of !!x!!?',\n",
        " 'DISTINCT_countrylanguage._\"CountryCode\"': 'What is the alpha-3 country code of !!x!!?',\n",
        " 'PROJECTION_SurfaceArea': 'What is the surface area of !!x!!?',\n",
        " \"FILTER_(Continent = 'North America ')\": 'Is !!x!! in North America?',\n",
        " 'FILTER_(SurfaceArea > CAST(3000 AS DOUBLE))': 'Does !!x!! have a surface area greater than 3000?',\n",
        " 'FILTER_(Population >= 160000)': 'Does !!x!! have a population greater than 160000?',\n",
        " 'FILTER_(Population <= 900000)': 'Does !!x!! have a population less than 900000?',\n",
        " 'SEQ_SCAN_airports': 'List some airports.',\n",
        " \"FILTER_(city = 'Goroka')\": 'Is !!x!! in Goroka?',\n",
        " 'PROJECTION_name': 'What is the name of !!x!!?',\n",
        " \"FILTER_(city = 'New York')\": 'Is !!x!! in New York?',\n",
        " 'PROJECTION_city': 'What is the city of !!x!!?',\n",
        " 'PROJECTION_country': 'What is the country of !!x!!?',\n",
        " 'PROJECTION_elevation': 'What is the elevation of !!x!!?',\n",
        " \"FILTER_(country = 'Iceland')\": 'Is !!x!! in Iceland?',\n",
        " 'AGGREGATE_PROJ_elevation': 'What is the elevation of !!x!!?',\n",
        " 'SEQ_SCAN_airlines': 'List some airlines.',\n",
        " \"FILTER_(name ~~ 'Orbit%')\": 'Does !!x!! start with \"Orbit\"?',\n",
        " 'FILTER_(elevation >= -50)': 'Is the elevation of !!x!! less than or equal to -50?',\n",
        " 'FILTER_(elevation <= 50)': 'Is the elevation of !!x!! greater than or equal to 50?',\n",
        " \"FILTER_(country = 'Greenland')\": 'Is !!x!! in Greenland?',\n",
        " 'AGGREGATE_PROJ_DISTINCT city': 'What is the city of !!x!!?',\n",
        " \"FILTER_(Airline = 'JetBlue Airways ')\": 'Is !!x!! the same as \"JetBlue Airways\"?',\n",
        " 'PROJECTION_Country': 'What is the country of !!x!!?',\n",
        " 'PROJECTION_Abbreviation': 'What is the abbreviation of !!x!!?',\n",
        " \"FILTER_(Country = 'USA')\": 'Is !!x!! in the USA?',\n",
        " 'PROJECTION_Airline': 'What is the airline of !!x!!?',\n",
        " 'SEQ_SCAN_flights': 'List some flights.',\n",
        " \"FILTER_(Abbreviation = 'UAL')\": 'Is \"UAL\" the abbreviation of !!x!!?',\n",
        " 'SEQ_SCAN_singer': 'List some singers.',\n",
        " \"FILTER_(Citizenship != 'France')\": 'Is the citizenship of !!x!! not French?',\n",
        " 'PROJECTION_GNP': 'What is the GNP of !!x!!?',\n",
        " \"FILTER_(Region = 'Southern Europe' )\": 'Is !!x!! in Southern Europe?',\n",
        " 'PROJECTION_Code': 'What is the alpha-3 country code of !!x!!?',\n",
        " 'FILTER_(population <= CAST(150000 AS BIGINT))': 'Is the population of !!x!! less than or equal to 150000?',\n",
        " 'FILTER_(elevation >= CAST(-50 AS BIGINT))':'Is the elevation of !!x!! greater than or equal to -50?',\n",
        " 'FILTER_(elevation <= CAST(50 AS BIGINT))':'Is the elevation of !!x!! less than or equal to 50?',\n",
        " 'FILTER_(Population >= CAST(160000 AS BIGINT))':'Is the population of !!x!! greater than or equal to 160000?',\n",
        " 'FILTER_(Population <= CAST(900000 AS BIGINT))':'Is the population of !!x!! less than or equal to 900000?',\n",
        " 'FILTER_(population > CAST(150000 AS BIGINT))':'Is the population of !!x!! greater than to 150000?',\n",
        " 'FILTER_(IndepYear > CAST(1950 AS BIGINT))':'Is the independence year of  !!x!! grater than 1950?',\n",
        " 'FILTER_(length > CAST(750 AS BIGINT))':'Is the length of !!x!! greater than 750 km?',\n",
        " \"FILTER_(Language = 'Dutch')\":'Is !!x!! the same as Dutch?',\n",
        " \"FILTER_(IsOfficial = CAST('T' AS BOOLEAN))\":'Does !!x!! have an official langauge?',\n",
        " \"FILTER_(HeadOfState = 'Beatrix')\":'Is Beatrix the head of state of !!x!!?',\n",
        " \"FILTER_(Name = 'Afghanistan')\":'Is !!x!! the same as Afghanistan?',\n",
        " \"FILTER_(Language = 'English')\":'Is !!x!! the same as English?'\n",
        "\n",
        " }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HOlscDWbmaK4"
      },
      "outputs": [],
      "source": [
        "#join_query_trees = []\n",
        "join_tree_nodes=[]\n",
        "\n",
        "def get_join_tree_nodes(root,a=[]):\n",
        "    if root:\n",
        "        get_join_tree_nodes(root.l,a)\n",
        "        get_join_tree_nodes(root.r,a)\n",
        "        a.append(root.text)\n",
        "    return a\n",
        "\n",
        "for q in join_query_trees:\n",
        "    a = get_join_tree_nodes(join_query_trees[q],[])\n",
        "    join_tree_nodes.append(a)\n",
        "\n",
        "flattened_tree_nodes = [xx for x in tree_nodes+join_tree_nodes for xx in x]\n",
        "flattened_tree_nodes = adjust_nodes_old(flattened_tree_nodes)\n",
        "\n",
        "\n",
        "augmented_question_maps = augment_questions(question_maps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr2UATA_BcOG"
      },
      "source": [
        "# **RUNNING CHATGPT MODEL - BASE 0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DOto4ZcuB0Xt"
      },
      "outputs": [],
      "source": [
        "inst_chatgpt = \"You are a highly intelligent question answering bot. If I ask you a question that is rooted in truth, you will give you the answer. If I ask you a question that is nonsense, trickery, or has no clear answer, you will respond with 'Unknown'. You will answer concisely.\"\n",
        "fewshot_chatgpt = [['What is human life expectancy in the United States?', '78.'],\n",
        " ['Who was president of the United States in 1955?', 'Dwight D. Eisenhower.'],\n",
        " ['Which party was founded by Gramsci?', 'Comunista.'],\n",
        " ['What is the capital of France?', 'Paris.'],\n",
        " ['What is a continent starting with letter O?', 'Oceania.'],\n",
        " ['Where were the 1992 Olympics held?', 'Barcelona.'],\n",
        " ['How many squigs are in a bonk?', 'Unknown']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HMT6pwT7AJ0M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def split_train_test(data, test_ratio):\n",
        "    df = list(data.items())\n",
        "    #df = [tup for tup in df in tup[0]]\n",
        "    random.shuffle(df)\n",
        "    test_size = int(len(df) * test_ratio)\n",
        "    return [[f\"{q}\", f\"{a}\"] for q, a in df[test_size:]], [[f\"{q}\", f\"{a}\"] for q, a in df[:test_size]]\n",
        "\n",
        "    #return [{\"input\": f\"{q}\", \"output\":f\"{a}\"} for q, a in df[test_size:]], [f\"{q} :--> {a}\" for q, a in df[:test_size]]\n",
        "\n",
        "dfTrain, dfTest = split_train_test(question_maps, 0.1)\n",
        "\n",
        "# Convert dataset into ChatGPT format\n",
        "dfTrain = construct_message_dict(inst_chatgpt, dfTrain)\n",
        "\n",
        "def prompt_create(dfTrain, q):\n",
        "  sleep(25)\n",
        "  print(f\"Prompt created for '{q}'!\")\n",
        "  answer = openai.ChatCompletion.create(\n",
        "          model='gpt-3.5-turbo',\n",
        "          messages=dfTrain + [construct_chat_dict(\"user\", q)],\n",
        "          temperature=0,\n",
        "          max_tokens=400\n",
        "  )\n",
        "  sleep(25)\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "Z4VnWnrtvIoE",
        "outputId": "266515c6-34f1-43e4-ad6f-f7d9c267c666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode:  w\n",
            "Checkpoint 1: Before the FOR loop\n",
            "\n",
            "Query: SELECT T1.name FROM continents as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  w\n",
            "['SEQ_SCAN', 'continents']\n",
            "Tree Nodes:  ['SEQ_SCAN_continents']\n",
            "Prompt created for 'SEQ_SCAN_continents'!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a9abec168c0f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mua1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT_SPW_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_arch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_DF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minst_chatgpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfew_shots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfewshot_chatgpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst_funct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Chat-GPT3-FS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmented_question_maps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmented_question_maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_plan_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin_query_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Questions: {q1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answers: {a1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unfiltered Answers: {ua1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bd74ac851783>\u001b[0m in \u001b[0;36mGPT_SPW_seq\u001b[0;34m(model_arch, df, instr, few_shots, inst_funct, label, augmented_question_maps, query_plan_dict, verbose)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# temp_questions=[]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mcompute_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfew_shots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minst_funct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maugmented_question_maps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0mtree_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munfiltered_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_snippet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bd74ac851783>\u001b[0m in \u001b[0;36mcompute_tree\u001b[0;34m(node, model_arch, instr, few_shots, inst_funct, label, augmented_question_maps, verbose)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mcompute_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfew_shots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minst_funct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maugmented_question_maps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcompute_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfew_shots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minst_funct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maugmented_question_maps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mcompute_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfew_shots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minst_funct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maugmented_question_maps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bd74ac851783>\u001b[0m in \u001b[0;36mcompute_node\u001b[0;34m(node, model_arch, instr, few_shots, inst_funct, label, augmented_question_maps, verbose)\u001b[0m\n\u001b[1;32m    275\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"k exists! k value is {k}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m               \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m               \u001b[0;31m# print(f\"question is: {question['choices'][0]['message']['content']}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;31m#question = node.filled_question if hasattr(node,'filled_question') else augmented_question_maps[k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-745a5273bb57>\u001b[0m in \u001b[0;36mprompt_create\u001b[0;34m(dfTrain, q)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prompt created for '{q}'!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   answer = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     21\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfTrain\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconstruct_chat_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             return (\n\u001b[0;32m--> 710\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    711\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ],
      "source": [
        "q1, a1, ua1 = GPT_SPW_seq(model_arch='gpt-3.5-turbo', df=create_DF, instr=inst_chatgpt, few_shots=fewshot_chatgpt, inst_funct=1, label='Chat-GPT3-FS', augmented_question_maps=augmented_question_maps, query_plan_dict=join_query_trees, verbose=True)\n",
        "\n",
        "print(f\"Questions: {q1}\")\n",
        "print(f\"Answers: {a1}\")\n",
        "print(f\"Unfiltered Answers: {ua1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK6sJ4G5f7ZV"
      },
      "outputs": [],
      "source": [
        "single_question_answers = [\n",
        "    completion_with_backoff_chat(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=construct_message_dict(inst_chatgpt, fewshot_chatgpt) + [construct_chat_dict(\"user\", question)],\n",
        "        temperature=0,\n",
        "        max_tokens=400\n",
        "    )['choices'][0]['message']['content']\n",
        "    for question in sel_DF['Question']\n",
        "]\n",
        "\n",
        "chatgpt_final_result_df = pd.DataFrame(single_question_answers, columns=['Single Question Answer'])\n",
        "chatgpt_final_result_df[['Question', 'Query Answer', 'Database', 'Query']] = sel_DF[['Question', 'Answer', 'Database', 'Query']]\n",
        "chatgpt_final_result_df = chatgpt_final_result_df[['Question', 'Query Answer', 'Database', 'Query', 'Single Question Answer']]\n",
        "chatgpt_final_result_df.to_csv('ChatGPT_50_Final.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoqNLft1Nasn"
      },
      "source": [
        "Questions: 1. Should I run all?\n",
        "2. Give examples for improvements from each?\n",
        "3.\n",
        "\n",
        "List of all experiments\n",
        "\n",
        "\n",
        "In he report, keep two columns of my prompts vs Muhammeds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH3s0AaYByE0"
      },
      "source": [
        "# **EXPERIMENT 1**\n",
        "To begin with, I consider augmented_question_maps as my dataset. I would like ChatGPT to infer similar relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61VlS_-PUYvZ"
      },
      "source": [
        "### **Fine Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g0wPfnSz74N",
        "outputId": "a59d2409-6fad-429e-e1b1-4f9ab66c96cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "251\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "print(len(dfTrain))\n",
        "print(len(dfTest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMKkFBls53vD",
        "outputId": "3f0a462d-0e44-4ef4-d7f1-75dc88ecfb48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt created for 'PROJECTION_GNP'!\n",
            "Predicted Answer: What is the GNP of !!x!!?\n",
            "True Answer: What is the GNP of !!x!!?\n",
            "Prompt created for 'SEQ_SCAN_river'!\n",
            "Predicted Answer: List some rivers.\n",
            "True Answer: List some rivers in the US.\n",
            "Prompt created for 'PROJECTION_Region'!\n",
            "Predicted Answer: What is the region of !!x!!?\n",
            "True Answer: What is the region of !!x!!?\n",
            "Prompt created for 'PROJECTION_river_name'!\n",
            "Predicted Answer: What is the name of the river of !!x!!?\n",
            "True Answer: WHat is the name of !!x!!?\n",
            "Prompt created for 'AGGREGATE_PROJ_state_name'!\n",
            "Predicted Answer: What is the name of the state of !!x!!?\n",
            "True Answer: What is the name of the state of !!x!!?\n",
            "Prompt created for 'PROJECTION_Population'!\n",
            "Predicted Answer: What is the population of !!x!!?\n",
            "True Answer: What is the population of !!x!!?\n",
            "Prompt created for 'AGGREGATE_PROJ_highest_elevation'!\n",
            "Predicted Answer: What is the highest elevation of !!x!!?\n",
            "True Answer: What is the highest elevation of !!x!!?\n",
            "Prompt created for 'FILTER_(Region = 'Southern Europe' )'!\n",
            "Predicted Answer: Is !!x!! in Southern Europe?\n",
            "True Answer: Is !!x!! in Southern Europe?\n",
            "Prompt created for 'FILTER_(state_name = 'california')'!\n",
            "Predicted Answer: Is !!x!! the same as California?\n",
            "True Answer: Is !!x!! the same as California?\n",
            "Prompt created for 'FILTER_(state_name = 'washington')'!\n",
            "Predicted Answer: Is !!x!! the same as Washington?\n",
            "True Answer: Is !!x!! in Washington?\n",
            "Prompt created for 'AGGREGATE_PROJ_capital'!\n"
          ]
        },
        {
          "ename": "APIError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5e518c91432d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgen_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mqa_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfTest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mpred_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predicted Answer: {pred_answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"True Answer: {qa_pair[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-745a5273bb57>\u001b[0m in \u001b[0;36mprompt_create\u001b[0;34m(dfTrain, q)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prompt created for '{q}'!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   answer = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     21\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfTrain\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconstruct_chat_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mAPIError\u001b[0m: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Thu, 29 Jun 2023 09:26:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ded19538b784a88-TPE', 'alt-svc': 'h3=\":443\"; ma=86400'}"
          ]
        }
      ],
      "source": [
        "\n",
        "gen_prompt = []\n",
        "for qa_pair in dfTest:\n",
        "  pred_answer = prompt_create(dfTrain, qa_pair[0])[\"choices\"][0][\"message\"][\"content\"]\n",
        "  print(f\"Predicted Answer: {pred_answer}\")\n",
        "  print(f\"True Answer: {qa_pair[1]}\")\n",
        "  gen_prompt.append({'Predicted Answer': pred_answer, 'True Answer': qa_pair[1]})\n",
        "  #sleep(21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su0IFpNuWDem"
      },
      "source": [
        "# **EXPERIMENT 2**\n",
        "\n",
        "Here, we try the question_maps completely generated by ChatGPT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ64ZQrlWIu8"
      },
      "source": [
        "### **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUeTUaO8WV8v",
        "outputId": "896d8cf9-21f2-498b-fbd0-f38dd3ef7eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'SEQ_SCAN_state': 'List some US states.'}\n"
          ]
        }
      ],
      "source": [
        "gpt_question_maps = {}\n",
        "for q in question_maps.keys():\n",
        "  a = prompt_create(dfTrain, q)[\"choices\"][0][\"message\"][\"content\"]\n",
        "  gpt_question_maps[q] = a\n",
        "  sleep(21)\n",
        "print(gpt_question_maps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7_QP2fO7D0j"
      },
      "source": [
        "### **GPT Calls**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeFcGl1b7KAJ"
      },
      "outputs": [],
      "source": [
        "GPT_SPW_seq(model_arch='gpt-3.5-turbo', df=sel_DF, instr=inst_chatgpt, few_shots=fewshot_chatgpt, inst_funct=1, label='Chat-GPT3-FS', augmented_question_maps=augment_questions(gpt_question_maps), query_plan_dict=join_query_trees, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myvY5L1qTyxI"
      },
      "source": [
        "# **EXPERIMENT 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dprFvKwfsHo"
      },
      "outputs": [],
      "source": [
        "\n",
        "question_maps = {'SEQ_SCAN_state': 'List the American states.',\n",
        " \"FILTER_(state_name = 'new mexico')\": 'Is !!x!! the same as \"New Mexico\"?',\n",
        " 'PROJECTION_area': 'What is the area of !!x!!?',\n",
        " \"FILTER_(state_name = 'california')\": 'Is !!x!! the same as California?',\n",
        " 'PROJECTION_population': 'What is the population of !!x!!?',\n",
        " 'SEQ_SCAN_city': 'List the cities.',\n",
        " \"FILTER_(state_name = 'texas')\": 'Is !!x!! the same as Texas?',\n",
        " 'PROJECTION_city_name': 'What is the name of !!x!!?',\n",
        " \"FILTER_(capital = 'albany')\": 'Is Albany the capital of !!x!!?',\n",
        " 'SEQ_SCAN_lake': 'List the American Lakes.',\n",
        " 'PROJECTION_lake_name': 'What is the name of the lake !!x!!?',\n",
        " 'FILTER_(area > CAST(750 AS DOUBLE) )': 'Is the area of !!x!! greater than 750 km2?',\n",
        " \"FILTER_(state_name = 'michigan')\": 'Is !!x!! in Michigan?',\n",
        " 'FILTER_(lowest_elevation = 0)': 'Is the lowest elevation in !!x!! 0 meters?',\n",
        " 'PROJECTION_highest_point': 'What is the highest point of !!x!!?',\n",
        " 'PROJECTION_state_name': 'What is the name of the state of !!x!!?',\n",
        " 'SEQ_SCAN_river': 'List the rivers in the US.',\n",
        " \"FILTER_(traverse = 'idaho')\": 'Does !!x!! traverse Idaho?',\n",
        " 'AGGREGATE_PROJ_river_name': 'What is the name of the river !!x!!?',\n",
        " \"FILTER_(traverse = 'illinois')\": 'Does !!x!! traverse Illinois?',\n",
        " 'PROJECTION_river_name': 'WHat is the name of !!x!!?',\n",
        " \"FILTER_(city_name = 'springfield')\": 'Is !!x!! the same as Springfield?',\n",
        " \"FILTER_(city_name = 'boulder')\": 'Is !!x!! the same as Boulder?',\n",
        " \"FILTER_(state_name = 'delaware')\": 'Is !!x!! in Delaware?',\n",
        " 'PROJECTION_highest_elevation': 'What is the highest elevation of !!x!!?',\n",
        " \"FILTER_(highest_point = 'guadalupe peak')\": 'Is guadalupe peak the highest point of !!x!!?',\n",
        " 'AGGREGATE_PROJ_highest_elevation': 'What is the highest elevation of !!x!!?',\n",
        " \"FILTER_(river_name = 'rio grande')\": 'Is !!x!! the same as \"Rio Grande\"?',\n",
        " 'PROJECTION_length': 'What is the length of !!x!!?',\n",
        " \"FILTER_(state_name = 'rhode island ')\": 'Is !!x!! the same as \"rhode island\"?',\n",
        " 'AGGREGATE_PROJ_capital': 'What is the capital of !!x!!?',\n",
        " 'AGGREGATE_PROJ_city_name': 'What is the name of !!x!!?',\n",
        " 'FILTER_(population > 150000)': 'Does !!x!! have a population greater than 150000?',\n",
        " \"FILTER_(river_name = 'colorado')\": 'Is !!x!! the same as Colorado?',\n",
        " \"FILTER_(city_name = 'seattle')\": 'Is !!x!! the same as Seattle?',\n",
        " \"FILTER_(state_name = 'washington')\": 'Is !!x!! in Washington?',\n",
        " 'AGGREGATE_PROJ_population': 'What is the popualtion of !!x!!?',\n",
        " 'AGGREGATE_PROJ_state_name': 'What is the name of the state of !!x!!?',\n",
        " 'FILTER_(length > 750)': 'Is the length of !!x!! greater than 750 miles?',\n",
        " 'PROJECTION_capital': 'What is the capital of !!x!!?',\n",
        " \"FILTER_(state_name = 'kansas')\": 'Is !!x!! in the state of Kanses?',\n",
        " \"FILTER_(state_name = 'wisconsin')\": 'Is !!x!! in the state of Wisconsin?',\n",
        " 'AGGREGATE_PROJ_area': 'What is the area of !!x!!?',\n",
        " \"FILTER_(state_name = 'wyoming')\": 'Is !!x!! the same as Wyoming?',\n",
        " 'PROJECTION_density': 'What is the density of !!x!!?',\n",
        " 'PROJECTION_lowest_point': 'What is the lowest point of !!x!!?',\n",
        " 'AGGREGATE_PROJ_length': 'What is the length of !!x!!?',\n",
        " 'PROJECTION_traverse': 'What states does !!x!! traverse?',\n",
        " 'SEQ_SCAN_mountain': 'List the mountains in the US.',\n",
        " \"FILTER_(mountain_name = 'whitney')\": 'Is !!x!! the same as \"Whitney\"?',\n",
        " \"FILTER_(city_name = 'austin')\": 'Is !!x!! the same as Austin?',\n",
        " \"FILTER_(capital = 'salem')\": 'Is Salem the capital of !!x!!?',\n",
        " \"FILTER_(river_name = 'missouri')\": 'Is !!x!! the same as \"missouri\"?',\n",
        " 'AGGREGATE_PROJ_traverse': 'What are the states that !!x!! traverses?',\n",
        " \"FILTER_(state_name = 'pennsylvania ')\": 'Is !!x!! the same as pennsylvania? ',\n",
        " 'PROJECTION_(CAST(population AS DOUBLE) / area)': 'What is the population of !!x!! divided by its area?',\n",
        " \"FILTER_(traverse = 'ohio')\": 'Does !!x!! traverse Ohio?',\n",
        " 'AGGREGATE_PROJ_DISTINCT traverse': 'What are the states that !!x!! traverses?',\n",
        " \"FILTER_(mountain_name = 'mckinley' )\": 'Is !!x!! the same as Mckinley?',\n",
        " 'PROJECTION_mountain_altitude': 'What is the altitude of !!x!!?',\n",
        " \"FILTER_(state_name = 'alaska')\": 'Is !!x!! in Alaska?',\n",
        " 'PROJECTION_mountain_name': 'What is the name of !!x!!?',\n",
        " \"FILTER_(state_name = 'massachusetts')\": 'Is !!x!! the same as massachusetts?',\n",
        " 'PROJECTION_country_name': 'What is the name of the country of !!x!!?',\n",
        " \"FILTER_(capital = 'austin')\": 'Is Austin the capital of !!x!!?',\n",
        " 'SEQ_SCAN_country': 'List the countries.',\n",
        " 'FILTER_(IndepYear > 1950)': 'Is the independence year of !!x!! greater than 1950?',\n",
        " 'PROJECTION_Name': 'What is the name of !!x!!?',\n",
        " \"FILTER_(GovernmentForm = 'Republic ')\": 'Is the government form of !!x!! republic?',\n",
        " 'AGGREGATE_count_star()': 'What is the count of the following list: !!list!!?',\n",
        " \"FILTER_(Region = 'Caribbean')\": 'Is !!x!! in the Caribbean?',\n",
        " 'AGGREGATE_PROJ_SurfaceArea': 'What is the surface area of !!x!!?',\n",
        " \"FILTER_(Name = 'Anguilla')\": 'Is !!x!! Anguilla?',\n",
        " 'PROJECTION_Continent': 'What is the continent of !!x!!?',\n",
        " \"FILTER_(Name = 'Brazil')\": 'Is !!x!! the same as Brazil?',\n",
        " 'PROJECTION_Population': 'What is the population of !!x!!?',\n",
        " 'PROJECTION_LifeExpectancy': 'What is the life expectancy of !!x!!?',\n",
        " \"FILTER_(Name = 'Angola')\": 'Is !!x!! Angola?',\n",
        " 'PROJECTION_Region': 'What is the region of !!x!!?',\n",
        " \"FILTER_(Region = 'Central Africa')\": 'Is !!x!! in central Africa?',\n",
        " 'AGGREGATE_PROJ_LifeExpectancy': 'What is the life expectancy of !!x!!?',\n",
        " \"FILTER_(Continent = 'Asia')\": 'Is !!x!! in Asia?',\n",
        " 'AGGREGATE_PROJ_Population': 'What is the population of !!x!!?',\n",
        " 'AGGREGATE_PROJ_GNP': 'What is the GNP of !!x!!?',\n",
        " \"FILTER_(Continent = 'Africa')\": 'Is !!x!! in Africa?',\n",
        " \"FILTER_(District = 'Gelderland')\": 'Is !!x!! in Gelderland district?',\n",
        " \"FILTER_(GovernmentForm = 'US Territory')\": 'Does !!x!! have US Territory as a form of government?',\n",
        " 'SEQ_SCAN_countrylanguage': 'List the languages.',\n",
        " 'AGGREGATE_PROJ_DISTINCT Language': 'What is the language of !!x!!?',\n",
        " 'AGGREGATE_PROJ_DISTINCT GovernmentForm': 'What is the government form of !!x!!?',\n",
        " \"FILTER_(Language != 'English')\": 'Is !!x!! not English?',\n",
        " 'PROJECTION_CountryCode': 'What is the alpha-3 country code of !!x!!?',\n",
        " 'DISTINCT_countrylanguage._\"CountryCode\"': 'What is the alpha-3 country code of !!x!!?',\n",
        " 'PROJECTION_SurfaceArea': 'What is the surface area of !!x!!?',\n",
        " \"FILTER_(Continent = 'North America ')\": 'Is !!x!! in North America?',\n",
        " 'FILTER_(SurfaceArea > CAST(3000 AS DOUBLE))': 'Does !!x!! have a surface area greater than 3000?',\n",
        " 'FILTER_(Population >= 160000)': 'Does !!x!! have a population greater than 160000?',\n",
        " 'FILTER_(Population <= 900000)': 'Does !!x!! have a population less than 900000?',\n",
        " 'SEQ_SCAN_airports': 'List the airports.',\n",
        " \"FILTER_(city = 'Goroka')\": 'Is !!x!! in Goroka?',\n",
        " 'PROJECTION_name': 'What is the name of !!x!!?',\n",
        " \"FILTER_(city = 'New York')\": 'Is !!x!! in New York?',\n",
        " 'PROJECTION_city': 'What is the city of !!x!!?',\n",
        " 'PROJECTION_country': 'What is the country of !!x!!?',\n",
        " 'PROJECTION_elevation': 'What is the elevation of !!x!!?',\n",
        " \"FILTER_(country = 'Iceland')\": 'Is !!x!! in Iceland?',\n",
        " 'AGGREGATE_PROJ_elevation': 'What is the elevation of !!x!!?',\n",
        " 'SEQ_SCAN_airlines': 'List the airlines.',\n",
        " \"FILTER_(name ~~ 'Orbit%')\": 'Does !!x!! start with \"Orbit\"?',\n",
        " 'FILTER_(elevation >= -50)': 'Is the elevation of !!x!! less than or equal to -50?',\n",
        " 'FILTER_(elevation <= 50)': 'Is the elevation of !!x!! greater than or equal to 50?',\n",
        " \"FILTER_(country = 'Greenland')\": 'Is !!x!! in Greenland?',\n",
        " 'AGGREGATE_PROJ_DISTINCT city': 'What is the city of !!x!!?',\n",
        " \"FILTER_(Airline = 'JetBlue Airways ')\": 'Is !!x!! the same as \"JetBlue Airways\"?',\n",
        " 'PROJECTION_Country': 'What is the country of !!x!!?',\n",
        " 'PROJECTION_Abbreviation': 'What is the abbreviation of !!x!!?',\n",
        " \"FILTER_(Country = 'USA')\": 'Is !!x!! in the USA?',\n",
        " 'PROJECTION_Airline': 'What is the airline of !!x!!?',\n",
        " 'SEQ_SCAN_flights': 'List the flights.',\n",
        " \"FILTER_(Abbreviation = 'UAL')\": 'Is \"UAL\" the abbreviation of !!x!!?',\n",
        " 'SEQ_SCAN_singer': 'List the singers.',\n",
        " \"FILTER_(Citizenship != 'France')\": 'Is the citizenship of !!x!! not French?',\n",
        " 'PROJECTION_GNP': 'What is the GNP of !!x!!?',\n",
        " \"FILTER_(Region = 'Southern Europe' )\": 'Is !!x!! in Southern Europe?',\n",
        " 'PROJECTION_Code': 'What is the alpha-3 country code of !!x!!?',\n",
        " 'FILTER_(population <= CAST(150000 AS BIGINT))': 'Is the population of !!x!! less than or equal to 150000?',\n",
        " 'FILTER_(elevation >= CAST(-50 AS BIGINT))':'Is the elevation of !!x!! greater than or equal to -50?',\n",
        " 'FILTER_(elevation <= CAST(50 AS BIGINT))':'Is the elevation of !!x!! less than or equal to 50?',\n",
        " 'FILTER_(Population >= CAST(160000 AS BIGINT))':'Is the population of !!x!! greater than or equal to 160000?',\n",
        " 'FILTER_(Population <= CAST(900000 AS BIGINT))':'Is the population of !!x!! less than or equal to 900000?',\n",
        " 'FILTER_(population > CAST(150000 AS BIGINT))':'Is the population of !!x!! greater than to 150000?',\n",
        " 'FILTER_(IndepYear > CAST(1950 AS BIGINT))':'Is the independence year of  !!x!! grater than 1950?',\n",
        " 'FILTER_(length > CAST(750 AS BIGINT))':'Is the length of !!x!! greater than 750 km?',\n",
        " \"FILTER_(Language = 'Dutch')\":'Is !!x!! the same as Dutch?',\n",
        " \"FILTER_(IsOfficial = CAST('T' AS BOOLEAN))\":'Does !!x!! have an official langauge?',\n",
        " \"FILTER_(HeadOfState = 'Beatrix')\":'Is Beatrix the head of state of !!x!!?',\n",
        " \"FILTER_(Name = 'Afghanistan')\":'Is !!x!! the same as Afghanistan?',\n",
        " \"FILTER_(Language = 'English')\":'Is !!x!! the same as English?'\n",
        "\n",
        " }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-KzNjhuT31h",
        "outputId": "053aa6bd-f5db-42e9-ada1-c6ad53c81b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mode:  r\n",
            "Checkpoint 1: Before the FOR loop\n",
            "\n",
            "Query: SELECT T1.name FROM continents as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  r\n",
            "['SEQ_SCAN', 'continents']\n",
            "Tree Nodes:  ['SEQ_SCAN_continents']\n",
            "Prompt created for 'SEQ_SCAN_continents'!\n",
            "OP:  SEQ_SCAN_continents\n",
            "Q:  List the continents. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\n",
            "Unfiltered Answer:  ['Africa, Antarctica, Asia, Europe, North America, Oceania, South America.Unknown.']\n",
            "Final Answer:  [' Asia', ' Antarctica', ' Europe', ' North America', 'Africa', ' Oceania']\n",
            "\n",
            "\n",
            "===================================================================================\n",
            "Query: SELECT T1.name FROM rainbow_colors as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  r\n",
            "['SEQ_SCAN', 'rainbow_colors']\n",
            "Tree Nodes:  ['SEQ_SCAN_rainbow_colors']\n",
            "Prompt created for 'SEQ_SCAN_rainbow_colors'!\n",
            "OP:  SEQ_SCAN_rainbow_colors\n",
            "Q:  List the colors of the rainbow. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\n",
            "Unfiltered Answer:  ['Red, orange, yellow, green, blue, indigo, violet.Unknown.']\n",
            "Final Answer:  [' blue', ' green', ' indigo', ' orange', 'Red', ' yellow']\n",
            "\n",
            "\n",
            "===================================================================================\n",
            "Query: SELECT T1.name FROM oceans as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  r\n",
            "['SEQ_SCAN', 'oceans']\n",
            "Tree Nodes:  ['SEQ_SCAN_oceans']\n",
            "Prompt created for 'SEQ_SCAN_oceans'!\n",
            "OP:  SEQ_SCAN_oceans\n",
            "Q:  List the oceans. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\n",
            "Unfiltered Answer:  ['Atlantic, Pacific, Indian, Southern, Arctic.Unknown.']\n",
            "Final Answer:  ['Atlantic', ' Pacific', ' Indian', ' Southern']\n",
            "\n",
            "\n",
            "===================================================================================\n",
            "Query: SELECT T1.name FROM zodiac_signs as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  r\n",
            "['SEQ_SCAN', 'zodiac_signs']\n",
            "Tree Nodes:  ['SEQ_SCAN_zodiac_signs']\n",
            "Prompt created for 'SEQ_SCAN_zodiac_signs'!\n",
            "OP:  SEQ_SCAN_zodiac_signs\n",
            "Q:  List the zodiac signs. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\n",
            "Unfiltered Answer:  ['Aries, Taurus, Gemini, Cancer, Leo, Virgo, Libra, Scorpio, Sagittarius, Capricorn, Aquarius, Pisces.Unknown.']\n",
            "Final Answer:  [' Cancer', ' Leo', ' Virgo', ' Libra', ' Scorpio', ' Gemini', 'Aries', ' Capricorn', ' Sagittarius', ' Taurus', ' Aquarius']\n",
            "\n",
            "\n",
            "===================================================================================\n",
            "Query: SELECT T1.name FROM planets as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  r\n",
            "['SEQ_SCAN', 'planets']\n",
            "Tree Nodes:  ['SEQ_SCAN_planets']\n",
            "Prompt created for 'SEQ_SCAN_planets'!\n",
            "OP:  SEQ_SCAN_planets\n",
            "Q:  List the planets. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\n",
            "Unfiltered Answer:  ['Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.Pluto (formerly considered a planet), Eris, Haumea, Makemake.']\n",
            "Final Answer:  ['Mercury', ' Makemake', ' Mars', ' Earth', ' Saturn', ' Uranus', ' Venus', ' Haumea', ' Eris', ' Jupiter']\n",
            "\n",
            "\n",
            "===================================================================================\n",
            "Query: SELECT T1.name FROM snow_white_dwarfs as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  r\n",
            "['SEQ_SCAN', 'snow_white_dwarfs']\n",
            "Tree Nodes:  ['SEQ_SCAN_snow_white_dwarfs']\n",
            "Prompt created for 'SEQ_SCAN_snow_white_dwarfs'!\n",
            "OP:  SEQ_SCAN_snow_white_dwarfs\n",
            "Q:  List the snow white dwarfs. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\n",
            "Unfiltered Answer:  ['Doc, Grumpy, Happy, Sleepy, Bashful, Sneezy, Dopey.Unknown.']\n",
            "Final Answer:  [' Sneezy', ' Bashful', 'Doc', ' Sleepy', ' Happy', ' Grumpy']\n",
            "\n",
            "\n",
            "===================================================================================\n",
            "Query: SELECT T1.name FROM deadly_sins as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  r\n",
            "['SEQ_SCAN', 'deadly_sins']\n",
            "Tree Nodes:  ['SEQ_SCAN_deadly_sins']\n",
            "Prompt created for 'SEQ_SCAN_deadly_sins'!\n",
            "OP:  SEQ_SCAN_deadly_sins\n",
            "Q:  List the deadly sins. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\n",
            "Unfiltered Answer:  ['Pride, greed, lust, envy, gluttony, wrath, sloth.Unknown.']\n",
            "Final Answer:  [' lust', ' envy', ' gluttony', ' greed', ' wrath', 'Pride']\n",
            "\n",
            "\n",
            "===================================================================================\n",
            "Query: SELECT T1.name FROM nobel_prize_categories as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  r\n",
            "['SEQ_SCAN', 'nobel_prize_categories']\n",
            "Tree Nodes:  ['SEQ_SCAN_nobel_prize_categories']\n",
            "Prompt created for 'SEQ_SCAN_nobel_prize_categories'!\n",
            "OP:  SEQ_SCAN_nobel_prize_categories\n",
            "Q:  List the Nobel Prize categories. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\n",
            "Unfiltered Answer:  ['Physics, Chemistry, Medicine, Literature, Peace, Economic Sciences.Sorry, I cannot provide more Nobel Prize categories as there are only six categories.']\n",
            "Final Answer:  [' I cannot provide more Nobel Prize categories as there are only six categories', ' Peace', 'Physics', ' Chemistry', ' Literature', ' Medicine']\n",
            "\n",
            "\n",
            "===================================================================================\n",
            "Query: SELECT T1.name FROM harry_potter_titles as T1\n",
            "\n",
            "\n",
            "Out Of The If/Else Block\n",
            "Node Questions: []\n",
            "Node Answers: []\n",
            "Mode:  r\n",
            "['SEQ_SCAN', 'harry_potter_titles']\n",
            "Tree Nodes:  ['SEQ_SCAN_harry_potter_titles']\n",
            "Prompt created for 'SEQ_SCAN_harry_potter_titles'!\n",
            "OP:  SEQ_SCAN_harry_potter_titles\n",
            "Q:  List the titles of the Harry Potter books. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.\n",
            "Unfiltered Answer:  [\"Harry Potter and the Philosopher's Stone, Harry Potter and the Chamber of Secrets, Harry Potter and the Prisoner of Azkaban, Harry Potter and the Goblet of Fire, Harry Potter and the Order of the Phoenix, Harry Potter and the Half-Blood Prince, Harry Potter and the Deathly Hallows.Unknown.\"]\n",
            "Final Answer:  [' Harry Potterthe Order of the Phoenix', \"Harry Potterthe Philosopher's Stone\", ' Harry Potterthe Prisoner of Azkaban', ' Harry Potterthe Chamber of Secrets', ' Harry Potterthe Half-Blood Prince', ' Harry Potterthe Goblet of Fire']\n",
            "\n",
            "\n",
            "===================================================================================\n",
            "Questions: [['List the continents. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.'], ['List the colors of the rainbow. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.'], ['List the oceans. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.'], ['List the zodiac signs. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.'], ['List the planets. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.'], ['List the snow white dwarfs. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.'], ['List the deadly sins. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.'], ['List the Nobel Prize categories. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.'], ['List the titles of the Harry Potter books. Don’t justify your answers. Don’t give information not asked in the CONTEXT INFORMATION.']]\n",
            "Answers: [[' Asia', ' Antarctica', ' Europe', ' North America', 'Africa', ' Oceania'], [' blue', ' green', ' indigo', ' orange', 'Red', ' yellow'], ['Atlantic', ' Pacific', ' Indian', ' Southern'], [' Cancer', ' Leo', ' Virgo', ' Libra', ' Scorpio', ' Gemini', 'Aries', ' Capricorn', ' Sagittarius', ' Taurus', ' Aquarius'], ['Mercury', ' Makemake', ' Mars', ' Earth', ' Saturn', ' Uranus', ' Venus', ' Haumea', ' Eris', ' Jupiter'], [' Sneezy', ' Bashful', 'Doc', ' Sleepy', ' Happy', ' Grumpy'], [' lust', ' envy', ' gluttony', ' greed', ' wrath', 'Pride'], [' I cannot provide more Nobel Prize categories as there are only six categories', ' Peace', 'Physics', ' Chemistry', ' Literature', ' Medicine'], [' Harry Potterthe Order of the Phoenix', \"Harry Potterthe Philosopher's Stone\", ' Harry Potterthe Prisoner of Azkaban', ' Harry Potterthe Chamber of Secrets', ' Harry Potterthe Half-Blood Prince', ' Harry Potterthe Goblet of Fire']]\n",
            "Unfiltered Answers: [['Africa, Antarctica, Asia, Europe, North America, Oceania, South America.Unknown.'], ['Red, orange, yellow, green, blue, indigo, violet.Unknown.'], ['Atlantic, Pacific, Indian, Southern, Arctic.Unknown.'], ['Aries, Taurus, Gemini, Cancer, Leo, Virgo, Libra, Scorpio, Sagittarius, Capricorn, Aquarius, Pisces.Unknown.'], ['Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.Pluto (formerly considered a planet), Eris, Haumea, Makemake.'], ['Doc, Grumpy, Happy, Sleepy, Bashful, Sneezy, Dopey.Unknown.'], ['Pride, greed, lust, envy, gluttony, wrath, sloth.Unknown.'], ['Physics, Chemistry, Medicine, Literature, Peace, Economic Sciences.Sorry, I cannot provide more Nobel Prize categories as there are only six categories.'], [\"Harry Potter and the Philosopher's Stone, Harry Potter and the Chamber of Secrets, Harry Potter and the Prisoner of Azkaban, Harry Potter and the Goblet of Fire, Harry Potter and the Order of the Phoenix, Harry Potter and the Half-Blood Prince, Harry Potter and the Deathly Hallows.Unknown.\"]]\n"
          ]
        }
      ],
      "source": [
        "inst_chatgpt = \"You are a highly intelligent question answering bot. I would like you to be measured with your replies. Please respond ONLY with the answers and no explanation. Limit your answers as much as possible. If I ask you a question that is rooted in truth, you will give you the answer. If I ask you a question that is nonsense, trickery, or has no clear answer, you will respond with 'Unknown'.\"\n",
        "fewshot_chatgpt = [['What is human life expectancy in the United States?', '78.'],\n",
        " ['Who was president of the United States in 1955?', 'Dwight D. Eisenhower.'],\n",
        " ['Which party was founded by Gramsci?', 'Comunista.'],\n",
        " ['What is the capital of France?', 'Paris.'],\n",
        " ['What is a continent starting with letter O?', 'Oceania.'],\n",
        " ['Where were the 1992 Olympics held?', 'Barcelona.'],\n",
        " ['How many squigs are in a bonk?', 'Unknown']]\n",
        "\n",
        "def split_train_test(data, test_ratio):\n",
        "    df = list(data.items())\n",
        "    #df = [tup for tup in df in tup[0]]\n",
        "    random.shuffle(df)\n",
        "    test_size = int(len(df) * test_ratio)\n",
        "    return [[f\"{q}\", f\"{a}\"] for q, a in df[test_size:]], [[f\"{q}\", f\"{a}\"] for q, a in df[:test_size]]\n",
        "\n",
        "    #return [{\"input\": f\"{q}\", \"output\":f\"{a}\"} for q, a in df[test_size:]], [f\"{q} :--> {a}\" for q, a in df[:test_size]]\n",
        "\n",
        "dfTrain, dfTest = split_train_test(question_maps, 0.1)\n",
        "\n",
        "# Convert dataset into ChatGPT format\n",
        "dfTrain = construct_message_dict(inst_chatgpt, dfTrain)\n",
        "\n",
        "def prompt_create(dfTrain, q):\n",
        "  sleep(25)\n",
        "  print(f\"Prompt created for '{q}'!\")\n",
        "  answer = openai.ChatCompletion.create(\n",
        "          model='gpt-3.5-turbo',\n",
        "          messages=dfTrain + [construct_chat_dict(\"user\", q)],\n",
        "          temperature=0,\n",
        "          max_tokens=400\n",
        "  )\n",
        "  sleep(25)\n",
        "  return answer\n",
        "\n",
        "q2, a2, ua2 = GPT_SPW_seq(model_arch='gpt-3.5-turbo', df=create_DF, instr=inst_chatgpt, few_shots=fewshot_chatgpt, inst_funct=1, label='Chat-GPT3-FS', augmented_question_maps=augmented_question_maps, query_plan_dict=join_query_trees, verbose=True)\n",
        "\n",
        "print(f\"Questions: {q2}\")\n",
        "print(f\"Answers: {a2}\")\n",
        "print(f\"Unfiltered Answers: {ua2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcVhJ3g1C_qj"
      },
      "source": [
        "# **RESULTS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tRd5p3turzR"
      },
      "source": [
        "## **Experiment 1 Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT5NxMxKYuak",
        "outputId": "8ee4ff00-232a-451b-97fb-a1d174a4e484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rouge-1\n",
            "r: 0.914918414918415\n",
            "p: 0.9231601731601733\n",
            "f: 0.9166935578650693\n",
            "\n",
            "\n",
            "rouge-2\n",
            "r: 0.823076923076923\n",
            "p: 0.8044871794871794\n",
            "f: 0.8091715930622724\n",
            "\n",
            "\n",
            "rouge-l\n",
            "r: 0.8951048951048951\n",
            "p: 0.9033466533466534\n",
            "f: 0.8968800380515496\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "hyps1, refs1 = map(list, zip(*[[d['Predicted Answer'], d['True Answer']] for d in gen_prompt]))\n",
        "\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(hyps1, refs1, avg=True)\n",
        "\n",
        "for method in scores.keys():\n",
        "  print(method)\n",
        "  for metric in scores[method].keys():\n",
        "    print(f\"{metric}: {scores[method][metric]}\")\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NHYCK4M0wEi"
      },
      "source": [
        "## **Experiment 2 Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuZfTQCyAGUG"
      },
      "outputs": [],
      "source": [
        "def stringify(input_list):\n",
        "    output_list = []\n",
        "    for el in input_list:\n",
        "      output_list.append(str(sorted(el)).lower())\n",
        "    return output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHxjDgRAwg7A",
        "outputId": "505446a5-ef7a-4e79-d051-4a301026c9be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['africa', 'antarctica', 'asia', 'australia', 'europe', 'north america', 'south america']\n",
            "['africa', 'antarctica', 'asia', 'australia/oceania', 'europe', 'north america']\n",
            "[{'rouge-1': {'r': 0.8571428571428571, 'p': 0.6666666666666666, 'f': 0.7499999950781251}, 'rouge-2': {'r': 0.5, 'p': 0.375, 'f': 0.4285714236734694}, 'rouge-l': {'r': 0.8571428571428571, 'p': 0.6666666666666666, 'f': 0.7499999950781251}}]\n",
            "\n",
            "\n",
            "['blue', 'green', 'indigo', 'orange', 'red', 'violet', 'yellow']\n",
            "['red', 'blue', 'green', 'indigo', 'orange', 'yellow']\n",
            "[{'rouge-1': {'r': 0.6666666666666666, 'p': 0.5714285714285714, 'f': 0.6153846104142012}, 'rouge-2': {'r': 0.4, 'p': 0.3333333333333333, 'f': 0.36363635867768596}, 'rouge-l': {'r': 0.6666666666666666, 'p': 0.5714285714285714, 'f': 0.6153846104142012}}]\n",
            "\n",
            "\n",
            "['arctic ocean', 'atlantic ocean', 'indian ocean', 'pacific ocean', 'southern ocean']\n",
            "['arctic ocean', 'atlantic ocean', 'indian ocean', 'pacific ocean', 'southern ocean']\n",
            "[{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n",
            "\n",
            "\n",
            "['aquarius', 'aries', 'cancer', 'capricorn', 'gemini', 'leo', 'libra', 'pisces', 'sagittarius', 'scorpio', 'taurus', 'virgo']\n",
            "['aquarius', 'aries', 'cancer', 'capricorn', 'gemini', 'leo', 'libra', 'sagittarius', 'scorpio', 'taurus', 'virgo']\n",
            "[{'rouge-1': {'r': 1.0, 'p': 0.9166666666666666, 'f': 0.9565217341398866}, 'rouge-2': {'r': 0.9, 'p': 0.8181818181818182, 'f': 0.8571428521541952}, 'rouge-l': {'r': 1.0, 'p': 0.9166666666666666, 'f': 0.9565217341398866}}]\n",
            "\n",
            "\n",
            "['earth', 'jupiter', 'mars', 'mercury', 'neptune', 'saturn', 'uranus', 'venus']\n",
            "[' earth', 'jupiter', 'mars', 'mercury', 'saturn', 'uranus', 'venus', 'there are also numerous exoplanets discovered outside our solar system']\n",
            "[{'rouge-1': {'r': 0.2777777777777778, 'p': 0.625, 'f': 0.38461538035502957}, 'rouge-2': {'r': 0.17647058823529413, 'p': 0.42857142857142855, 'f': 0.2499999958680556}, 'rouge-l': {'r': 0.2777777777777778, 'p': 0.625, 'f': 0.38461538035502957}}]\n",
            "\n",
            "\n",
            "['bashful', 'biggy', 'bossy', 'doc', 'dopey', 'grimm', 'grumpy', 'happy', 'humbert', 'sleepy', 'smoky', 'sneezy']\n",
            "[]\n",
            "[{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n",
            "\n",
            "\n",
            "['envy', 'gluttony', 'greed', 'lust', 'pride', 'sloth', 'wrath']\n",
            "['pride', 'envy', 'gluttony', 'greed', \"it's worth noting that different culturesreligions may have different interpretations or additional sins\", 'lust', 'wrath']\n",
            "[{'rouge-1': {'r': 0.2222222222222222, 'p': 0.5714285714285714, 'f': 0.31999999596800005}, 'rouge-2': {'r': 0.05555555555555555, 'p': 0.16666666666666666, 'f': 0.0833333295833335}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.5714285714285714, 'f': 0.31999999596800005}}]\n",
            "\n",
            "\n",
            "['chemistry', 'literature', 'medicine', 'peace', 'physics']\n",
            "['chemistry', 'literature', 'medicine', 'peace', 'physics', 'but i can only provide the six nobel prize categories that are currently recognized']\n",
            "[{'rouge-1': {'r': 0.21052631578947367, 'p': 0.8, 'f': 0.33333333003472226}, 'rouge-2': {'r': 0.16666666666666666, 'p': 0.75, 'f': 0.2727272697520661}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.8, 'f': 0.33333333003472226}}]\n",
            "\n",
            "\n",
            "['harry potter and the chamber of secrets', 'harry potter and the deathly hallows', 'harry potter and the goblet of fire', 'harry potter and the half-blood prince', 'harry potter and the order of the phoenix', \"harry potter and the philosopher's stone\", 'harry potter and the prisoner of azkaban']\n",
            "[' but they are not part of the main book series', '\"harry potterthe philosopher\\'s stone\"\\n\"harry potterthe chamber of secrets\"\\n\"harry potterthe prisoner of azkaban\"\\n\"harry potterthe goblet of fire\"\\n\"harry potterthe order of the phoenix\"\\n\"harry potterthe half-blood prince\"\\n\"harry potterthe deathly hallows\"i apologize for the confusion']\n",
            "[{'rouge-1': {'r': 0.26666666666666666, 'p': 0.38095238095238093, 'f': 0.31372548535178785}, 'rouge-2': {'r': 0.125, 'p': 0.16666666666666666, 'f': 0.14285713795918387}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.23809523809523808, 'f': 0.19607842652825846}}]\n",
            "\n",
            "\n",
            "rouge-1\n",
            "r: 0.5001113895850737\n",
            "p: 0.6146825396825396\n",
            "f: 0.5192867251490836\n",
            "\n",
            "\n",
            "rouge-2\n",
            "r: 0.3692992011619462\n",
            "p: 0.4487133237133237\n",
            "f: 0.3775853736297766\n",
            "\n",
            "\n",
            "rouge-l\n",
            "r: 0.48900027847396266\n",
            "p: 0.5988095238095238\n",
            "f: 0.506214829724247\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Q = ['List some continents.', 'List some colors of the rainbow.', 'List some oceans.', 'List some zodiac signs.', 'List some planets.', 'List some snow white dwarfs.', 'List some deadly sins.', 'List some Nobel Prize categories.', 'List some Harry Potter titles.']\n",
        "RA = [['Africa', 'Asia', 'Europe', 'North America', 'South America', 'Australia', 'Antarctica'], ['Red', 'Orange', 'Yellow', 'Green', 'Blue', 'Indigo', 'Violet'], ['Pacific Ocean', 'Atlantic Ocean', 'Indian Ocean', 'Southern Ocean', 'Arctic Ocean'], ['Aries', 'Taurus', 'Gemini', 'Cancer', 'Leo', 'Virgo', 'Libra', 'Scorpio', 'Sagittarius', 'Capricorn', 'Aquarius', 'Pisces'], ['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune'], ['Doc', 'Grumpy', 'Happy', 'Sleepy', 'Bashful', 'Sneezy', 'Dopey', 'Humbert', 'Grimm', 'Bossy', 'Biggy', 'Smoky'], ['Lust', 'Gluttony', 'Greed', 'Sloth', 'Wrath', 'Envy', 'Pride'], ['Physics', 'Chemistry', 'Medicine', 'Literature', 'Peace'], [\"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Order of the Phoenix', 'Harry Potter and the Half-Blood Prince', 'Harry Potter and the Deathly Hallows']]\n",
        "UFA = [['Africa, Antarctica, Asia, Europe, North America, Australia/Oceania, South America.I apologize for the oversight. Here are some additional continents: \\n- Zealandia (sometimes considered a submerged continent)\\n- Afro-Eurasia (combination of Africa and Eurasia)\\n- Americas (combination of North and South America)'], ['Red, orange, yellow, green, blue, indigo, violet.Unknown.'], ['Atlantic Ocean, Pacific Ocean, Indian Ocean, Southern Ocean, Arctic Ocean.There are only five recognized oceans: Atlantic Ocean, Pacific Ocean, Indian Ocean, Southern Ocean, and Arctic Ocean.'], ['Aries, Taurus, Gemini, Cancer, Leo, Virgo, Libra, Scorpio, Sagittarius, Capricorn, Aquarius, Pisces.Unknown.'], ['Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.Pluto (formerly considered a planet), and there are also numerous exoplanets discovered outside our solar system.'], ['Unknown.Unknown.'], [\"Pride, greed, lust, envy, gluttony, wrath, and sloth.Those are the seven deadly sins recognized in Christian teachings. However, it's worth noting that different cultures and religions may have different interpretations or additional sins.\"], ['Physics, Chemistry, Medicine, Literature, Peace, Economic Sciences.Sorry, but I can only provide the six Nobel Prize categories that are currently recognized.'], ['\"Harry Potter and the Philosopher\\'s Stone\"\\n\"Harry Potter and the Chamber of Secrets\"\\n\"Harry Potter and the Prisoner of Azkaban\"\\n\"Harry Potter and the Goblet of Fire\"\\n\"Harry Potter and the Order of the Phoenix\"\\n\"Harry Potter and the Half-Blood Prince\"\\n\"Harry Potter and the Deathly Hallows\"I apologize for the confusion, but there are only seven main Harry Potter books as listed above. There are additional companion books and screenplays related to the Harry Potter series, but they are not part of the main book series.']]\n",
        "FA = [['Europe', 'Asia', 'Australia/Oceania', 'Africa', 'North America', 'Antarctica'], ['green', 'orange', 'blue', 'yellow', 'Red', 'indigo'], ['Indian Ocean', 'Atlantic Ocean', 'Southern Ocean', 'Pacific Ocean', 'Arctic Ocean'], ['Cancer', 'Aquarius', 'Sagittarius', 'Scorpio', 'Libra', 'Capricorn', 'Virgo', 'Leo', 'Aries', 'Taurus', 'Gemini'], ['Mercury', 'Saturn', 'Uranus', 'Jupiter', 'Venus', 'Mars', 'there are also numerous exoplanets discovered outside our solar system', ' Earth'], [], ['gluttony', 'Pride', \"it's worth noting that different culturesreligions may have different interpretations or additional sins\", 'envy', 'greed', 'wrath', 'lust'], ['Medicine', 'Physics', 'but I can only provide the six Nobel Prize categories that are currently recognized', 'Literature', 'Peace', 'Chemistry'], ['\"Harry Potterthe Philosopher\\'s Stone\"\\n\"Harry Potterthe Chamber of Secrets\"\\n\"Harry Potterthe Prisoner of Azkaban\"\\n\"Harry Potterthe Goblet of Fire\"\\n\"Harry Potterthe Order of the Phoenix\"\\n\"Harry Potterthe Half-Blood Prince\"\\n\"Harry Potterthe Deathly Hallows\"I apologize for the confusion', ' but they are not part of the main book series']]\n",
        "\n",
        "\n",
        "#hyps1, refs1 = map(list, zip(*[stringify(RA), stringify(UFA)]))\n",
        "#print(hyps1)\n",
        "rouge = Rouge()\n",
        "\n",
        "for i in range(0, len(RA)):\n",
        "  print(stringify(RA)[i])\n",
        "  print(stringify(FA)[i])\n",
        "  print(rouge.get_scores(stringify(RA)[i], stringify(FA)[i]))\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "exp2_scores = rouge.get_scores(stringify(RA), stringify(FA), avg=True)\n",
        "\n",
        "for method in exp2_scores.keys():\n",
        "  print(method)\n",
        "  for metric in exp2_scores[method].keys():\n",
        "    print(f\"{metric}: {exp2_scores[method][metric]}\")\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "024Q-HX-ghY3"
      },
      "source": [
        "## **Experiment 3 Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO5UeVf-_xcc",
        "outputId": "d781dd18-a358-43d7-e95e-b9e5d27925ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['africa', 'antarctica', 'asia', 'australia', 'europe', 'north america', 'south america']\n",
            "['africa', 'antarctica', 'asia', 'europe', 'north america', 'oceania']\n",
            "[{'rouge-1': {'r': 0.8571428571428571, 'p': 0.6666666666666666, 'f': 0.7499999950781251}, 'rouge-2': {'r': 0.6666666666666666, 'p': 0.5, 'f': 0.5714285665306124}, 'rouge-l': {'r': 0.8571428571428571, 'p': 0.6666666666666666, 'f': 0.7499999950781251}}]\n",
            "\n",
            "\n",
            "['blue', 'green', 'indigo', 'orange', 'red', 'violet', 'yellow']\n",
            "['red', 'blue', 'green', 'indigo', 'orange', 'yellow']\n",
            "[{'rouge-1': {'r': 0.6666666666666666, 'p': 0.5714285714285714, 'f': 0.6153846104142012}, 'rouge-2': {'r': 0.4, 'p': 0.3333333333333333, 'f': 0.36363635867768596}, 'rouge-l': {'r': 0.6666666666666666, 'p': 0.5714285714285714, 'f': 0.6153846104142012}}]\n",
            "\n",
            "\n",
            "['arctic ocean', 'atlantic ocean', 'indian ocean', 'pacific ocean', 'southern ocean']\n",
            "['atlantic ocean', 'indian ocean', 'pacific ocean', 'southern ocean']\n",
            "[{'rouge-1': {'r': 0.8333333333333334, 'p': 0.7142857142857143, 'f': 0.7692307642603551}, 'rouge-2': {'r': 0.8571428571428571, 'p': 0.6666666666666666, 'f': 0.7499999950781251}, 'rouge-l': {'r': 0.8333333333333334, 'p': 0.7142857142857143, 'f': 0.7692307642603551}}]\n",
            "\n",
            "\n",
            "['aquarius', 'aries', 'cancer', 'capricorn', 'gemini', 'leo', 'libra', 'pisces', 'sagittarius', 'scorpio', 'taurus', 'virgo']\n",
            "['aquarius', 'aries', 'cancer', 'capricorn', 'gemini', 'leo', 'libra', 'sagittarius', 'scorpio', 'taurus', 'virgo']\n",
            "[{'rouge-1': {'r': 1.0, 'p': 0.9166666666666666, 'f': 0.9565217341398866}, 'rouge-2': {'r': 0.9, 'p': 0.8181818181818182, 'f': 0.8571428521541952}, 'rouge-l': {'r': 1.0, 'p': 0.9166666666666666, 'f': 0.9565217341398866}}]\n",
            "\n",
            "\n",
            "['earth', 'jupiter', 'mars', 'mercury', 'neptune', 'saturn', 'uranus', 'venus']\n",
            "['earth', 'eris', 'haumea', 'jupiter', 'makemake', 'mars', 'mercury', 'saturn', 'uranus', 'venus']\n",
            "[{'rouge-1': {'r': 0.7, 'p': 0.875, 'f': 0.7777777728395062}, 'rouge-2': {'r': 0.3333333333333333, 'p': 0.42857142857142855, 'f': 0.37499999507812504}, 'rouge-l': {'r': 0.7, 'p': 0.875, 'f': 0.7777777728395062}}]\n",
            "\n",
            "\n",
            "['bashful', 'biggy', 'bossy', 'doc', 'dopey', 'grimm', 'grumpy', 'happy', 'humbert', 'sleepy', 'smoky', 'sneezy']\n",
            "['bashful', 'doc', 'grumpy', 'happy', 'sleepy', 'sneezy']\n",
            "[{'rouge-1': {'r': 1.0, 'p': 0.5, 'f': 0.6666666622222223}, 'rouge-2': {'r': 0.2, 'p': 0.09090909090909091, 'f': 0.12499999570312517}, 'rouge-l': {'r': 1.0, 'p': 0.5, 'f': 0.6666666622222223}}]\n",
            "\n",
            "\n",
            "['envy', 'gluttony', 'greed', 'lust', 'pride', 'sloth', 'wrath']\n",
            "['pride', 'envy', 'gluttony', 'greed', 'lust', 'wrath']\n",
            "[{'rouge-1': {'r': 0.6666666666666666, 'p': 0.5714285714285714, 'f': 0.6153846104142012}, 'rouge-2': {'r': 0.4, 'p': 0.3333333333333333, 'f': 0.36363635867768596}, 'rouge-l': {'r': 0.6666666666666666, 'p': 0.5714285714285714, 'f': 0.6153846104142012}}]\n",
            "\n",
            "\n",
            "['chemistry', 'literature', 'medicine', 'peace', 'physics']\n",
            "['chemistry', 'i cannot provide more nobel prize categories as there are only six categories', 'literature', 'medicine', 'peace', 'physics']\n",
            "[{'rouge-1': {'r': 0.2777777777777778, 'p': 1.0, 'f': 0.4347826052930058}, 'rouge-2': {'r': 0.17647058823529413, 'p': 0.75, 'f': 0.2857142826303855}, 'rouge-l': {'r': 0.2777777777777778, 'p': 1.0, 'f': 0.4347826052930058}}]\n",
            "\n",
            "\n",
            "['harry potter and the chamber of secrets', 'harry potter and the deathly hallows', 'harry potter and the goblet of fire', 'harry potter and the half-blood prince', 'harry potter and the order of the phoenix', \"harry potter and the philosopher's stone\", 'harry potter and the prisoner of azkaban']\n",
            "['harry potterthe chamber of secrets', 'harry potterthe goblet of fire', 'harry potterthe half-blood prince', 'harry potterthe order of the phoenix', \"harry potterthe philosopher's stone\", 'harry potterthe prisoner of azkaban']\n",
            "[{'rouge-1': {'r': 0.9444444444444444, 'p': 0.8095238095238095, 'f': 0.8717948668244576}, 'rouge-2': {'r': 0.64, 'p': 0.5333333333333333, 'f': 0.5818181768595042}, 'rouge-l': {'r': 0.9444444444444444, 'p': 0.8095238095238095, 'f': 0.8717948668244576}}]\n",
            "\n",
            "\n",
            "rouge-1\n",
            "r: 0.7717813051146385\n",
            "p: 0.736111111111111\n",
            "f: 0.7175048468317735\n",
            "\n",
            "\n",
            "rouge-2\n",
            "r: 0.5081792717086835\n",
            "p: 0.49492544492544494\n",
            "f: 0.47481962015438267\n",
            "\n",
            "\n",
            "rouge-l\n",
            "r: 0.7717813051146385\n",
            "p: 0.736111111111111\n",
            "f: 0.7175048468317735\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "RA = [['Africa', 'Asia', 'Europe', 'North America', 'South America', 'Australia', 'Antarctica'], ['Red', 'Orange', 'Yellow', 'Green', 'Blue', 'Indigo', 'Violet'], ['Pacific Ocean', 'Atlantic Ocean', 'Indian Ocean', 'Southern Ocean', 'Arctic Ocean'], ['Aries', 'Taurus', 'Gemini', 'Cancer', 'Leo', 'Virgo', 'Libra', 'Scorpio', 'Sagittarius', 'Capricorn', 'Aquarius', 'Pisces'], ['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune'], ['Doc', 'Grumpy', 'Happy', 'Sleepy', 'Bashful', 'Sneezy', 'Dopey', 'Humbert', 'Grimm', 'Bossy', 'Biggy', 'Smoky'], ['Lust', 'Gluttony', 'Greed', 'Sloth', 'Wrath', 'Envy', 'Pride'], ['Physics', 'Chemistry', 'Medicine', 'Literature', 'Peace'], [\"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Order of the Phoenix', 'Harry Potter and the Half-Blood Prince', 'Harry Potter and the Deathly Hallows']]\n",
        "FA = [['Asia', 'Antarctica', 'Europe', 'North America', 'Africa', 'Oceania'], ['blue', 'green', 'indigo', 'orange', 'Red', 'yellow'], ['Atlantic', 'Pacific', 'Indian', 'Southern'], ['Cancer', 'Leo', 'Virgo', 'Libra', 'Scorpio', 'Gemini', 'Aries', 'Capricorn', 'Sagittarius', 'Taurus', 'Aquarius'], ['Mercury', 'Makemake', 'Mars', 'Earth', 'Saturn', 'Uranus', 'Venus', 'Haumea', 'Eris', 'Jupiter'], ['Sneezy', 'Bashful', 'Doc', 'Sleepy', 'Happy', 'Grumpy'], ['lust', 'envy', 'gluttony', 'greed', 'wrath', 'Pride'], ['I cannot provide more Nobel Prize categories as there are only six categories', 'Peace', 'Physics', 'Chemistry', 'Literature', 'Medicine'], ['Harry Potterthe Order of the Phoenix', \"Harry Potterthe Philosopher's Stone\", 'Harry Potterthe Prisoner of Azkaban', 'Harry Potterthe Chamber of Secrets', 'Harry Potterthe Half-Blood Prince', 'Harry Potterthe Goblet of Fire']]\n",
        "FA2 = [['Asia', 'Antarctica', 'Europe', 'North America', 'Africa', 'Oceania'], ['blue', 'green', 'indigo', 'orange', 'Red', 'yellow'], ['Atlantic Ocean', 'Pacific Ocean', 'Indian Ocean', 'Southern Ocean'], ['Cancer', 'Leo', 'Virgo', 'Libra', 'Scorpio', 'Gemini', 'Aries', 'Capricorn', 'Sagittarius', 'Taurus', 'Aquarius'], ['Mercury', 'Makemake', 'Mars', 'Earth', 'Saturn', 'Uranus', 'Venus', 'Haumea', 'Eris', 'Jupiter'], ['Sneezy', 'Bashful', 'Doc', 'Sleepy', 'Happy', 'Grumpy'], ['lust', 'envy', 'gluttony', 'greed', 'wrath', 'Pride'], ['I cannot provide more Nobel Prize categories as there are only six categories', 'Peace', 'Physics', 'Chemistry', 'Literature', 'Medicine'], ['Harry Potterthe Order of the Phoenix', \"Harry Potterthe Philosopher's Stone\", 'Harry Potterthe Prisoner of Azkaban', 'Harry Potterthe Chamber of Secrets', 'Harry Potterthe Half-Blood Prince', 'Harry Potterthe Goblet of Fire']]\n",
        "UFA = [['Africa, Antarctica, Asia, Europe, North America, Oceania, South America.Unknown.'], ['Red, orange, yellow, green, blue, indigo, violet.Unknown.'], ['Atlantic, Pacific, Indian, Southern, Arctic.Unknown.'], ['Aries, Taurus, Gemini, Cancer, Leo, Virgo, Libra, Scorpio, Sagittarius, Capricorn, Aquarius, Pisces.Unknown.'], ['Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.Pluto (formerly considered a planet), Eris, Haumea, Makemake.'], ['Doc, Grumpy, Happy, Sleepy, Bashful, Sneezy, Dopey.Unknown.'], ['Pride, greed, lust, envy, gluttony, wrath, sloth.Unknown.'], ['Physics, Chemistry, Medicine, Literature, Peace, Economic Sciences.Sorry, I cannot provide more Nobel Prize categories as there are only six categories.'], [\"Harry Potter and the Philosopher's Stone, Harry Potter and the Chamber of Secrets, Harry Potter and the Prisoner of Azkaban, Harry Potter and the Goblet of Fire, Harry Potter and the Order of the Phoenix, Harry Potter and the Half-Blood Prince, Harry Potter and the Deathly Hallows.Unknown.\"]]\n",
        "\n",
        "#print(stringify(RA))\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "for i in range(0, len(RA)):\n",
        "  print(stringify(RA)[i])\n",
        "  print(stringify(FA2)[i])\n",
        "  print(rouge.get_scores(stringify(RA)[i], stringify(FA2)[i]))\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "exp3_scores = rouge.get_scores(stringify(RA), stringify(FA2), avg=True)\n",
        "\n",
        "for method in exp3_scores.keys():\n",
        "  print(method)\n",
        "  for metric in exp3_scores[method].keys():\n",
        "    print(f\"{metric}: {exp3_scores[method][metric]}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "# for ra, fa, scores in zip(stringify(RA), stringify(FA), scores):\n",
        "#   print(ra)\n",
        "#   print(fa)\n",
        "#   print(score)\n",
        "#   print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZNKmijT1wVF"
      },
      "source": [
        "# **Code Dump**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqlrC1WPTPUk"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Set up your OpenAI API credentials\n",
        "openai.api_key = 'YOUR_API_KEY'\n",
        "\n",
        "# Define your training data\n",
        "training_data = [\n",
        "    {\n",
        "        'question': 'What is the capital of France?',\n",
        "        'answer': 'Paris'\n",
        "    },\n",
        "    {\n",
        "        'question': 'What is the capital of Germany?',\n",
        "        'answer': 'Berlin'\n",
        "    },\n",
        "    # Add more question-answer pairs as needed\n",
        "]\n",
        "\n",
        "# Prepare the training data in the prompt and continuation format\n",
        "formatted_data = ''\n",
        "for data in training_data:\n",
        "    formatted_data += f'Q: {data[\"question\"]}\\nA: {data[\"answer\"]}\\n'\n",
        "\n",
        "# Define the fine-tuning parameters\n",
        "fine_tuning_params = {\n",
        "    'model': 'gpt-3.5-turbo',\n",
        "    'dataset': {\n",
        "        'prompt': formatted_data,\n",
        "        'completion': None\n",
        "    },\n",
        "    'max_tokens': 1000,\n",
        "    'n_epochs': 3,\n",
        "    'learning_rate': 1e-5,\n",
        "    'batch_size': 4,\n",
        "    'overwrite_dataset': True,\n",
        "    'validation_split': 0.1,\n",
        "    'validation_every_n_epochs': 1\n",
        "}\n",
        "\n",
        "# Start the fine-tuning process\n",
        "response = openai.ChatCompletion.create(**fine_tuning_params)\n",
        "\n",
        "# Get the fine-tuned model ID\n",
        "model_id = response['id']\n",
        "\n",
        "# Wait for the fine-tuning to complete\n",
        "while True:\n",
        "    model = openai.ChatCompletion.fetch(model_id)\n",
        "    if model['status'] == 'ready':\n",
        "        break\n",
        "\n",
        "# Use the fine-tuned model for inference\n",
        "def infer_relationships(question):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model_id,\n",
        "        messages=[\n",
        "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "            {'role': 'user', 'content': question}\n",
        "        ],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test the model by providing a question\n",
        "question = 'What is the capital of Italy?'\n",
        "inferred_relationship = infer_relationships(question)\n",
        "print(f\"Inferred Relationship: {inferred_relationship}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvP2zoUqCUgO",
        "outputId": "949e8167-68b8-4278-d3b8-d13165556b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            Question  \\\n",
            "0  What is the number of distinct continents wher...   \n",
            "1         Which region is the city Kabul located in?   \n",
            "\n",
            "                       Query Answer Database  \\\n",
            "0                            [(4,)]  world_1   \n",
            "1  [('Southern and Central Asia',)]  world_1   \n",
            "\n",
            "                                               Query  \\\n",
            "0  SELECT COUNT( DISTINCT Continent) FROM country...   \n",
            "1  SELECT Region FROM country AS T1 JOIN city AS ...   \n",
            "\n",
            "                    Single Question Answer  \n",
            "0                                     One.  \n",
            "1  Kabul is located in the region of Asia.  \n"
          ]
        }
      ],
      "source": [
        "#print(sel_DF)\n",
        "print(chatgpt_final_result_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRlNHTGnDCJE"
      },
      "outputs": [],
      "source": [
        "galois_ans = json.load(open('Chat-GPT3-FS.json','r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "DdTcqvmnDJhE",
        "outputId": "e09096e1-f498-47f0-fa43-735204ddc902"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4582bc114e4f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Question :{galois_ans[i][\"Gold Question\"]}\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Query :{galois_ans[i][\"Query\"]}\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "\n",
        "\n",
        "print(f'Question :{galois_ans[i][\"Gold Question\"]}\\n\\n')\n",
        "\n",
        "print(f'Query :{galois_ans[i][\"Query\"]}\\n\\n')\n",
        "\n",
        "print(f'Status :{galois_ans[i][\"Status\"]}\\n\\n')\n",
        "\n",
        "\n",
        "tree_nodes = galois_ans[i][\"Tree Nodes\"]\n",
        "\n",
        "for k,tn in enumerate(tree_nodes):\n",
        "    print(f'Tree Node:{tn}\\n\\n')\n",
        "\n",
        "    print(f'Tree Node Question:{galois_ans[i][\"LP Questions\"][k]}\\n\\n')\n",
        "\n",
        "    print(f'Tree Node Unfiltered Answers:{galois_ans[i][\"LP Unfiltered Answers\"][k]}\\n\\n')\n",
        "\n",
        "    print(f'Tree Node Answers:{galois_ans[i][\"LP Answers\"][k]}\\n\\n')\n",
        "\n",
        "    print('===============================================================================')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "1labN08DDP5f",
        "outputId": "0653b2d5-8fd7-4ff0-93b8-e1ecb3030208"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-913cbb2c0a14>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ChatGPT 50 Queries - Remove non-parsed(3).csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfinal_sample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final_50_Sample.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtype_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_sample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Type2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ChatGPT 50 Queries - Remove non-parsed(3).csv'"
          ]
        }
      ],
      "source": [
        "results_df = pd.read_csv('ChatGPT 50 Queries - Remove non-parsed(3).csv')\n",
        "final_sample_df = pd.read_csv('Final_50_Sample.csv')\n",
        "type_df = final_sample_df[['Question','Type2']]\n",
        "results_df = results_df.merge(type_df,how='left',on='Question')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nb7N1EoDWZY"
      },
      "outputs": [],
      "source": [
        "def get_final_type(x):\n",
        "    if 'J' in x: return 'Join'\n",
        "    if 'A' in x: return 'Agg'\n",
        "    return 'Sel'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "4DaCAsJwDZUy",
        "outputId": "68ff9892-7d6d-46b7-8e54-f35430a9d394"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1b6cdc4f3410>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Final Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mType2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_final_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
          ]
        }
      ],
      "source": [
        "results_df['Final Type']=results_df.Type2.apply(lambda x: get_final_type(x))\n",
        "results_df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgllE9lhCx7l"
      },
      "outputs": [],
      "source": [
        "#rompt = \"I want you to help me translate some snippets into a format that I define by example:\\n\" + \"\\n\".join(dfTrain)\n",
        "\n",
        "#print(prompt)\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=dfTrain,\n",
        "    max_tokens=100,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    temperature=0.7,\n",
        "    prompt_loss_weight=0.0,  # Use 0.0 for no influence on prompt style\n",
        "    metadata={\"language\": \"en\"}  # Adjust metadata as needed\n",
        ")\n",
        "\n",
        "\n",
        "# Fine-tune the model\n",
        "# response = openai.Completion.create(\n",
        "#     engine='text-davinci-003',  # Use an appropriate engine for fine-tuning\n",
        "#     prompt=prompt,\n",
        "#     max_tokens=3500,  # Adjust as needed\n",
        "#     n=1,  # Generate a single response\n",
        "#     stop=None,  # Stop condition if required\n",
        "#     temperature=0,  # Control randomness of the output\n",
        "#     top_p=1.0,  # Control the diversity of the output\n",
        "#     frequency_penalty=0.8,  # Adjust if too repetitive\n",
        "#     presence_penalty=0.1,  # Adjust if too verbose\n",
        "# )\n",
        "\n",
        "model_id = response['model']\n",
        "print(response['choices'])\n",
        "\n",
        "\n",
        "model_file_path = \"trained_model.txt\"\n",
        "with open(model_file_path, \"w\") as f:\n",
        "    f.write(response[\"model\"])\n",
        "\n",
        "# Load the trained model for further testing or conversations\n",
        "with open(model_file_path, \"r\") as f:\n",
        "    trained_model = f.read()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Z-s8s8a8z9uR",
        "j-sdfiMqaeLh",
        "s_DgWzPcak03",
        "X4gVPVP8jpIr",
        "mZNKmijT1wVF"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}